{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeQivnZVaf8_"
      },
      "outputs": [],
      "source": [
        "# Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡ ØªØ³Øª\n",
        "sage_test_acc, sage_test_f1 = evaluate(sage_model, data, test_idx)\n",
        "gcn_test_acc, gcn_test_f1 = evaluate(gcn_model, data, test_idx)\n",
        "\n",
        "print(f'GraphSAGE - Test Accuracy: {sage_test_acc:.4f}, Test F1: {sage_test_f1:.4f}')\n",
        "print(f'GCN - Test Accuracy: {gcn_test_acc:.4f}, Test F1: {gcn_test_f1:.4f}')\n",
        "\n",
        "# Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(sage_train_loss, label='GraphSAGE')\n",
        "plt.plot(gcn_train_loss, label='GCN')\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(sage_val_acc, label='GraphSAGE Val Acc')\n",
        "plt.plot(gcn_val_acc, label='GCN Val Acc')\n",
        "plt.title('Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuTpIVJJavYp"
      },
      "source": [
        "# Edge Pr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8hs4xDtatwO"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import LinkPrediction\n",
        "from torch_geometric.utils import negative_sampling\n",
        "\n",
        "class EdgePredictor(torch.nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.lin1 = torch.nn.Linear(2 * in_channels, 128)\n",
        "        self.lin2 = torch.nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, z, edge_index):\n",
        "        src, dst = edge_index\n",
        "        x = torch.cat([z[src], z[dst]], dim=1)\n",
        "        x = F.relu(self.lin1(x))\n",
        "        return torch.sigmoid(self.lin2(x)).view(-1)\n",
        "\n",
        "# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² embeddings ÛŒØ§Ø¯Ú¯Ø±ÙØªÙ‡ Ø´Ø¯Ù‡ ØªÙˆØ³Ø· GraphSAGE\n",
        "edge_model = EdgePredictor(hidden_channels)\n",
        "optimizer = torch.optim.Adam(edge_model.parameters(), lr=0.01)\n",
        "\n",
        "# Ø¢Ù…ÙˆØ²Ø´ edge prediction\n",
        "for epoch in range(50):\n",
        "    edge_model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # ØªÙˆÙ„ÛŒØ¯ embeddings Ø¨Ø§ GraphSAGE\n",
        "    z = sage_model.conv1(data.x, data.edge_index)\n",
        "    z = sage_model.conv2(z, data.edge_index)\n",
        "\n",
        "    # Ù†Ù…ÙˆÙ†Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø² ÛŒØ§Ù„Ù‡Ø§ÛŒ Ù…Ø«Ø¨Øª Ùˆ Ù…Ù†ÙÛŒ\n",
        "    pos_edge_index = data.edge_index\n",
        "    neg_edge_index = negative_sampling(data.edge_index, num_nodes=data.num_nodes)\n",
        "\n",
        "    # Ù…Ø­Ø§Ø³Ø¨Ù‡ loss\n",
        "    pos_pred = edge_model(z, pos_edge_index)\n",
        "    neg_pred = edge_model(z, neg_edge_index)\n",
        "\n",
        "    pos_loss = -torch.log(pos_pred + 1e-15).mean()\n",
        "    neg_loss = -torch.log(1 - neg_pred + 1e-15).mean()\n",
        "    loss = pos_loss + neg_loss\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hc8Zf6tcI86"
      },
      "source": [
        "-----------------------------\n",
        "# Temp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ù†ØµØ¨ numpy Ø³Ø§Ø²Ú¯Ø§Ø±\n",
        "!pip install numpy==1.24.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "7dRLBy1Z64nB",
        "outputId": "7373d7f7-2c42-49ba-b731-969034ed221b",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.24.4\n",
            "  Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pymc 5.23.0 requires numpy>=1.25.0, but you have numpy 1.24.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.4 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "xarray-einstats 0.9.0 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.4 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.4 which is incompatible.\n",
            "blosc2 3.3.4 requires numpy>=1.26, but you have numpy 1.24.4 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "1e752b19079e4d99a31e56b3eb89edfd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ù†ØµØ¨ Ù†Ø³Ø®Ù‡ Ø³Ø§Ø²Ú¯Ø§Ø± PyTorch\n",
        "!pip install torch==2.0.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ø¨Ø³ØªÙ‡ Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ù…Ø·Ø§Ø¨Ù‚ PyG ØªÙˆØµÛŒÙ‡â€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ torch==2.0.1\n",
        "!pip install pyg-lib==0.2.0 torch-scatter==2.1.1 torch-sparse==0.6.17 torch-cluster==1.6.1 torch-spline-conv==1.2.2 -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
        "\n",
        "# Ù†ØµØ¨ Ù†Ø³Ø®Ù‡ ØµØ­ÛŒØ­ PyG\n",
        "!pip install torch-geometric==2.3.1\n",
        "\n",
        "# Ù†ØµØ¨ OGB\n",
        "!pip install ogb\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSuy5VjbgKNV",
        "outputId": "f4b08367-4ead-4955-c4b4-126807a5e625",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.0.1\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp311-cp311-linux_x86_64.whl (2267.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m514.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (4.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.1.6)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1) (3.31.6)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1)\n",
            "  Downloading https://download.pytorch.org/whl/lit-15.0.7.tar.gz (132 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.24.4)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.22.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.20.1%2Bcu118-cp311-cp311-linux_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.20.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.19.1%2Bcu118-cp311-cp311-linux_x86_64.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.19.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is still looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.18.1%2Bcu118-cp311-cp311-linux_x86_64.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m99.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.18.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.17.2%2Bcu118-cp311-cp311-linux_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.17.1%2Bcu118-cp311-cp311-linux_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.17.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.32.3)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.16.2%2Bcu118-cp311-cp311-linux_x86_64.whl (6.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.16.1%2Bcu118-cp311-cp311-linux_x86_64.whl (6.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.16.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.15.2%2Bcu118-cp311-cp311-linux_x86_64.whl (6.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (6.6 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.5.1%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.5.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.4.1%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.4.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is still looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.3.1%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.3.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.2.2%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.2.1%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.2.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.1.2%2Bcu118-cp311-cp311-linux_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.1.1%2Bcu118-cp311-cp311-linux_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.1.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.0.2%2Bcu118-cp311-cp311-linux_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "Building wheels for collected packages: lit\n",
            "  Building wheel for lit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-15.0.7-py3-none-any.whl size=89990 sha256=123b7503cd0029012cac780c380233d75745a427a58cc0fa2b774ba4a4ba9d2b\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/5d/45/34fe9945d5e45e261134e72284395be36c2d4828af38e2b0fe\n",
            "Successfully built lit\n",
            "Installing collected packages: lit, triton, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.6.0+cu124\n",
            "    Uninstalling torchaudio-2.6.0+cu124:\n",
            "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Successfully installed lit-15.0.7 torch-2.0.1+cu118 torchaudio-2.0.2+cu118 torchvision-0.15.2+cu118 triton-2.0.0\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
            "Collecting pyg-lib==0.2.0\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/pyg_lib-0.2.0%2Bpt20cu118-cp311-cp311-linux_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-scatter==2.1.1\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_scatter-2.1.1%2Bpt20cu118-cp311-cp311-linux_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-sparse==0.6.17\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_sparse-0.6.17%2Bpt20cu118-cp311-cp311-linux_x86_64.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-cluster==1.6.1\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_cluster-1.6.1%2Bpt20cu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-spline-conv==1.2.2\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_spline_conv-1.2.2%2Bpt20cu118-cp311-cp311-linux_x86_64.whl (886 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m886.5/886.5 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse==0.6.17) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse==0.6.17) (1.24.4)\n",
            "Installing collected packages: torch-spline-conv, torch-scatter, pyg-lib, torch-sparse, torch-cluster\n",
            "Successfully installed pyg-lib-0.2.0+pt20cu118 torch-cluster-1.6.1+pt20cu118 torch-scatter-2.1.1+pt20cu118 torch-sparse-0.6.17+pt20cu118 torch-spline-conv-1.2.2+pt20cu118\n",
            "Collecting torch-geometric==2.3.1\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.3.1) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.3.1) (1.24.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.3.1) (1.15.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.3.1) (3.1.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.3.1) (2.32.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.3.1) (3.2.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.3.1) (1.6.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.3.1) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric==2.3.1) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.3.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.3.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.3.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.3.1) (2025.4.26)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->torch-geometric==2.3.1) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->torch-geometric==2.3.1) (3.6.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910496 sha256=2fc712500f301632cc39a4e58efe97f10b8a0cb7d8314a245c78acefec5a6718\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/78/6c/9fd091ca1c5e137c66cbd03696ffa14f75e9abc5abfe0dbcc6\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.3.1\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (1.24.4)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (1.6.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.4.0)\n",
            "Collecting outdated>=0.2.0 (from ogb)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.11/dist-packages (from outdated>=0.2.0->ogb) (75.2.0)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
            "  Downloading littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from outdated>=0.2.0->ogb) (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (4.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.1.6)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb) (3.31.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb) (15.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->ogb) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Downloading littleutils-0.2.4-py3-none-any.whl (8.1 kB)\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.4 ogb-1.3.6 outdated-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "from torch_geometric.utils import to_undirected\n",
        "\n",
        "# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯ÛŒØªØ§Ø³Øª ogbn-products\n",
        "dataset = PygNodePropPredDataset(name='ogbn-products')\n",
        "data = dataset[0]  # ÙÙ‚Ø· ÛŒÚ© Ø´ÛŒØ¡ Data Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯\n",
        "\n",
        "# ÛŒØ§Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø¯ÙˆÙ† Ø¬Ù‡Øª Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…\n",
        "data.edge_index = to_undirected(data.edge_index)\n",
        "\n",
        "# ØªØ¨Ø¯ÛŒÙ„ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ Ø¨Ù‡ [num_nodes]\n",
        "data.y = data.y.squeeze()\n",
        "\n",
        "# Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ØŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒØŒ Ø¢Ø²Ù…ÙˆÙ†\n",
        "split_idx = dataset.get_idx_split()\n",
        "train_idx = split_idx['train']\n",
        "val_idx = split_idx['valid']\n",
        "test_idx = split_idx['test']\n",
        "\n",
        "# Ø¨Ø±Ø±Ø³ÛŒ Ø§ÙˆÙ„ÛŒÙ‡\n",
        "print(data)\n",
        "print(f\"# Train samples: {train_idx.shape[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtZDu6LLxbsx",
        "outputId": "db401219-b4b9-4eca-9819-34c8a41a6cfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This will download 1.38GB. Will you proceed? (y/N)\n",
            "y\n",
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/products.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 1.38 GB: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1414/1414 [00:28<00:00, 49.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/products.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 3536.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(num_nodes=2449029, edge_index=[2, 123718152], x=[2449029, 100], y=[2449029])\n",
            "# Train samples: 196615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ø³Ø±ÛŒØ¹ Ø³Ø§Ø²:"
      ],
      "metadata": {
        "id": "Dpnp7UMqVkQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import NeighborLoader\n",
        "from torch import tensor\n",
        "\n",
        "# data = dataset[0]\n",
        "# data.edge_index = to_undirected(data.edge_index)\n",
        "# data.y = data.y.squeeze()\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# # ÙÙ‚Ø· Ù‡Ù…ÛŒÙ† Ø¨Ø®Ø´ Ø±ÙˆÛŒ GPU ÛŒØ§ CPU Ù…Ù†ØªÙ‚Ù„ Ù…ÛŒØ´Ù‡\n",
        "# data = data.to(device)\n",
        "\n",
        "train_idx = split_idx['train'].clone().detach()\n",
        "val_idx = split_idx['valid'].clone().detach()\n",
        "test_idx = split_idx['test'].clone().detach()\n",
        "\n",
        "train_loader = NeighborLoader(\n",
        "    data,\n",
        "    input_nodes=train_idx,\n",
        "    num_neighbors=[5, 3],\n",
        "    batch_size=256,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_loader = NeighborLoader(\n",
        "    data,\n",
        "    input_nodes=val_idx,\n",
        "    num_neighbors=[5, 3],\n",
        "    batch_size=256\n",
        ")\n",
        "\n",
        "test_loader = NeighborLoader(\n",
        "    data,\n",
        "    input_nodes=test_idx,\n",
        "    num_neighbors=[5, 3],\n",
        "    batch_size=256\n",
        ")\n"
      ],
      "metadata": {
        "id": "o0GDZfFqRtEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GCN"
      ],
      "metadata": {
        "id": "_BNOvm_tEXG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "# ØªØ¹Ø±ÛŒÙ Ù…Ø¯Ù„ GCN Ø¯Ùˆ Ù„Ø§ÛŒÙ‡\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, num_features, hidden_channels, num_classes):\n",
        "        super(GCN, self).__init__()\n",
        "        # Ù„Ø§ÛŒÙ‡ Ø§ÙˆÙ„: Ø§Ø² ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ù‡ ÙØ¶Ø§ÛŒ Ù¾Ù†Ù‡Ø§Ù†\n",
        "        self.conv1 = GCNConv(num_features, hidden_channels, bias=False)\n",
        "        # Ù„Ø§ÛŒÙ‡ Ø¯ÙˆÙ…: Ø§Ø² ÙØ¶Ø§ÛŒ Ù¾Ù†Ù‡Ø§Ù† Ø¨Ù‡ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§\n",
        "        self.conv2 = GCNConv(hidden_channels, num_classes, bias=True)\n",
        "\n",
        "    # def forward(self, data):\n",
        "    #     x, edge_index = data.x, data.edge_index\n",
        "    #     x = self.conv1(x, edge_index)\n",
        "    #     x = F.relu(x)\n",
        "    #     x = F.dropout(x, p=0.5, training=self.training)\n",
        "    #     x = self.conv2(x, edge_index)\n",
        "    #     return x\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Dropout Ú©Ù…ØªØ± Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª (Ùˆ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø§ÙØª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¯Ø± Ø¯ÛŒØªØ§ÛŒ Ø²ÛŒØ§Ø¯)\n",
        "        x = F.dropout(x, p=0.3, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Bwoo2by8D7Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMQwc7qrFQ2z",
        "outputId": "520fc8a7-063e-4dd3-bfae-bc2ed4e3c4d2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ù…Ù‚Ø§Ø¯ÛŒØ± Ø§Ø² Ø¯ÛŒØªØ§Ø³Øª\n",
        "num_features = data.num_node_features       # =100\n",
        "num_classes = int(data.y.max().item()) + 1  # =47\n",
        "hidden_channels = 32                       # Ù‚Ø§Ø¨Ù„ ØªÙ†Ø¸ÛŒÙ…\n",
        "\n",
        "#-----------------------------------------------------\n",
        "save_path = \"/content/drive/MyDrive/gcn_node_last.pt\"\n",
        "#-----------------------------------------------------\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Ø³Ø§Ø®Øª Ù…Ø¯Ù„\n",
        "model = GCN(num_features, hidden_channels, num_classes).to(device)\n",
        "data = data.to(device)\n",
        "train_idx = train_idx.to(device)\n",
        "val_idx = val_idx.to(device)\n",
        "test_idx = test_idx.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "#----------------------------------------------------- Ú¯ÙˆÚ¯Ù„ Ú©ÙˆÙ„Ø¨\n",
        "start_epoch = 1\n",
        "try:\n",
        "    # Ø§Ú¯Ø± ÙØ§ÛŒÙ„ÛŒ Ø§Ø² Ù‚Ø¨Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯ØŒ Ø§Ø² Ø§Ø¯Ø§Ù…Ù‡ Ø§Ø¬Ø±Ø§ Ú©Ù†\n",
        "    checkpoint = torch.load(\"gcn_node_last.pt\")\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    print(f\"âœ… Ø§Ø¯Ø§Ù…Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø§Ø² epoch {start_epoch}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"â³ Ø¢Ù…ÙˆØ²Ø´ Ø§Ø² Ø§ÙˆÙ„ Ø´Ø±ÙˆØ¹ Ù…ÛŒâ€ŒØ´ÙˆØ¯\")\n",
        "\n",
        "#----------------------------------------------------- Ú¯ÙˆÚ¯Ù„ Ø¯Ø±Ø§ÛŒÙˆ\n",
        "\n",
        "start_epoch = 1\n",
        "import os\n",
        "if os.path.exists(save_path):\n",
        "    checkpoint = torch.load(save_path)\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    print(f\"âœ… Ø§Ø¯Ø§Ù…Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø§Ø² epoch {start_epoch}\")\n",
        "else:\n",
        "    print(\"ğŸŸ¡ ÙØ§ÛŒÙ„ Ù…Ø¯Ù„ Ù‚Ø¨Ù„ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø¢Ù…ÙˆØ²Ø´ Ø§Ø² Ø§Ø¨ØªØ¯Ø§ Ø¢ØºØ§Ø² Ù…ÛŒâ€ŒØ´ÙˆØ¯.\")\n",
        "#-----------------------------------------------------\n",
        "\n",
        "# def train():\n",
        "#     model.train()\n",
        "#     optimizer.zero_grad()\n",
        "#     out = model(data)\n",
        "#     loss = F.cross_entropy(out[train_idx], data.y[train_idx])\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     return loss.item()\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch.x, batch.edge_index)\n",
        "        loss = F.cross_entropy(out, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "# @torch.no_grad()\n",
        "# def test():\n",
        "#     model.eval()\n",
        "#     out = model(data)\n",
        "#     pred = out.argmax(dim=1)\n",
        "\n",
        "#     accs = []\n",
        "#     for idx in [train_idx, val_idx, test_idx]:\n",
        "#         correct = (pred[idx] == data.y[idx]).sum().item()\n",
        "#         acc = correct / idx.shape[0]\n",
        "#         accs.append(acc)\n",
        "#     return accs\n",
        "@torch.no_grad()\n",
        "def test(loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        out = model(batch.x, batch.edge_index)\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += (pred == batch.y).sum().item()\n",
        "        total += batch.y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "\n",
        "final_epoch = 3  # ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø§Ù¾ÙˆÚ©â€ŒÙ‡Ø§\n",
        "\n",
        "for epoch in range(start_epoch, final_epoch + 1):\n",
        "    loss = train()\n",
        "    train_acc = test(train_loader)\n",
        "    val_acc = test(val_loader)\n",
        "    test_acc = test(test_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch:03d}, Loss: {loss:.4f}, \"\n",
        "          f\"Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), f\"gcn_node_epoch_{epoch:03d}.pt\")\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict()\n",
        "    }, save_path)\n",
        "    print(f\"ğŸ’¾ Ù…Ø¯Ù„ Ø¯Ø± Google Drive Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: epoch {epoch}\")\n",
        "\n",
        "    # Ù¾Ø§ÛŒØ§Ù† Ø¢Ù…ÙˆØ²Ø´\n",
        "    if epoch == final_epoch:\n",
        "        print(\" \")\n",
        "\n",
        "print(\"âœ… Ø¢Ù…ÙˆØ²Ø´ Ú©Ø§Ù…Ù„ Ø´Ø¯ Ùˆ Ù…Ø¯Ù„ Ù†Ù‡Ø§ÛŒÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª.\")"
      ],
      "metadata": {
        "id": "NaaE9NsJ4j5-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85da3894-5463-4323-f653-43864110e72f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â³ Ø¢Ù…ÙˆØ²Ø´ Ø§Ø² Ø§ÙˆÙ„ Ø´Ø±ÙˆØ¹ Ù…ÛŒâ€ŒØ´ÙˆØ¯\n",
            "âœ… Ø§Ø¯Ø§Ù…Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø§Ø² epoch 4\n",
            "âœ… Ø¢Ù…ÙˆØ²Ø´ Ú©Ø§Ù…Ù„ Ø´Ø¯ Ùˆ Ù…Ø¯Ù„ Ù†Ù‡Ø§ÛŒÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U scikit-learn --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VlKF3ar1jlz5",
        "outputId": "034d9817-9c61-4e71-a55d-06e6e8ad7534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_gcn_all_metrics(model, loader, name=\"GCN\"):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        out = model(batch.x, batch.edge_index)\n",
        "        pred = out.argmax(dim=1)\n",
        "        all_preds.append(pred.cpu())\n",
        "        all_labels.append(batch.y.cpu())\n",
        "\n",
        "    y_true = torch.cat(all_labels).numpy()\n",
        "    y_pred = torch.cat(all_preds).numpy()\n",
        "\n",
        "    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
        "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "    print(f\"\\nğŸ“Š Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ {name}:\")\n",
        "    print(f\"Accuracy    : {acc:.4f}\")\n",
        "    print(f\"F1-Score(Macro): {f1_macro:.4f}\")\n",
        "    print(f\"F1-Score(Micro): {f1_micro:.4f}\")\n",
        "    print(f\"Precision   : {precision:.4f}\")\n",
        "    print(f\"Recall      : {recall:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"Accuracy\": acc,\n",
        "        \"F1-Macro\": f1_macro,\n",
        "        \"F1-Micro\": f1_micro,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall\n",
        "    }\n",
        "\n",
        "# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ GCN Ùˆ Ù„ÙˆØ¯Ø± ØªØ³Øª\n",
        "gcn_metrics = evaluate_gcn_all_metrics(model, test_loader, name=\"GCN\")\n",
        "evaluate_gcn_all_metrics(model, val_loader, name=\"GCN-Validation\")\n",
        "evaluate_gcn_all_metrics(model, train_loader, name=\"GCN-Train\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbTpQ6i0jIdw",
        "outputId": "32b94434-1e0d-4fd7-ff01-334f15e7134a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Š Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ GCN:\n",
            "Accuracy    : 0.5621\n",
            "F1-Score(Macro): 0.1960\n",
            "F1-Score(Micro): 0.5621\n",
            "Precision   : 0.2872\n",
            "Recall      : 0.1905\n",
            "\n",
            "ğŸ“Š Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ GCN-Validation:\n",
            "Accuracy    : 0.6560\n",
            "F1-Score(Macro): 0.2094\n",
            "F1-Score(Micro): 0.6560\n",
            "Precision   : 0.3084\n",
            "Recall      : 0.1943\n",
            "\n",
            "ğŸ“Š Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ GCN-Train:\n",
            "Accuracy    : 0.6537\n",
            "F1-Score(Macro): 0.2111\n",
            "F1-Score(Micro): 0.6537\n",
            "Precision   : 0.3139\n",
            "Recall      : 0.1961\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Accuracy': 0.6537282071309946,\n",
              " 'F1-Macro': 0.21113551812103565,\n",
              " 'F1-Micro': 0.6537282071309946,\n",
              " 'Precision': 0.31394407493096366,\n",
              " 'Recall': 0.1960981341524284}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ø¨Ù‡Ø¨ÙˆØ¯ :"
      ],
      "metadata": {
        "id": "h8K-QLAcbzm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, BatchNorm1d\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class OptimizedGCN(torch.nn.Module):\n",
        "    def __init__(self, num_features, hidden_channels, num_classes):\n",
        "        super().__init__()\n",
        "        self.input_proj = Linear(num_features, 64)\n",
        "        self.conv1 = GCNConv(64, hidden_channels)\n",
        "        self.bn1 = BatchNorm1d(hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.input_proj(x)\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.4, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "fTjuUc0OUPNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, BatchNorm\n",
        "\n",
        "class BetterGCN(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.bn1 = BatchNorm(hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.bn2 = BatchNorm(hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.gelu(self.bn1(x))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.gelu(self.bn2(x))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        x = self.conv3(x, edge_index)\n",
        "        return x\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(data.y.cpu()), y=data.y.cpu())\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n"
      ],
      "metadata": {
        "id": "nIbHPXFfa7Gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ù…Ù‚Ø§Ø¯ÛŒØ± Ø§Ø² Ø¯ÛŒØªØ§Ø³Øª\n",
        "num_features = data.num_node_features       # =100\n",
        "num_classes = int(data.y.max().item()) + 1  # =47\n",
        "hidden_channels = 32                       # Ù‚Ø§Ø¨Ù„ ØªÙ†Ø¸ÛŒÙ…\n",
        "\n",
        "#-----------------------------------------------------\n",
        "save_path = \"/content/drive/MyDrive/gcn_node_betterWay.pt\"\n",
        "#-----------------------------------------------------\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Ø³Ø§Ø®Øª Ù…Ø¯Ù„\n",
        "model = BetterGCN(num_features, hidden_channels, num_classes).to(device)\n",
        "out = model(data.x, data.edge_index)\n",
        "loss = criterion(out[train_idx], data.y[train_idx])\n",
        "\n",
        "data = data.to(device)\n",
        "train_idx = train_idx.to(device)\n",
        "val_idx = val_idx.to(device)\n",
        "test_idx = test_idx.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "#----------------------------------------------------- Ú¯ÙˆÚ¯Ù„ Ú©ÙˆÙ„Ø¨\n",
        "start_epoch = 1\n",
        "try:\n",
        "    # Ø§Ú¯Ø± ÙØ§ÛŒÙ„ÛŒ Ø§Ø² Ù‚Ø¨Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯ØŒ Ø§Ø² Ø§Ø¯Ø§Ù…Ù‡ Ø§Ø¬Ø±Ø§ Ú©Ù†\n",
        "    checkpoint = torch.load(\"gcn_node_betterWay.pt\")\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    print(f\"âœ… Ø§Ø¯Ø§Ù…Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø§Ø² epoch {start_epoch}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"â³ Ø¢Ù…ÙˆØ²Ø´ Ø§Ø² Ø§ÙˆÙ„ Ø´Ø±ÙˆØ¹ Ù…ÛŒâ€ŒØ´ÙˆØ¯\")\n",
        "\n",
        "#----------------------------------------------------- Ú¯ÙˆÚ¯Ù„ Ø¯Ø±Ø§ÛŒÙˆ\n",
        "\n",
        "start_epoch = 1\n",
        "import os\n",
        "if os.path.exists(save_path):\n",
        "    checkpoint = torch.load(save_path)\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    print(f\"âœ… Ø§Ø¯Ø§Ù…Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø§Ø² epoch {start_epoch}\")\n",
        "else:\n",
        "    print(\"ğŸŸ¡ ÙØ§ÛŒÙ„ Ù…Ø¯Ù„ Ù‚Ø¨Ù„ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø¢Ù…ÙˆØ²Ø´ Ø§Ø² Ø§Ø¨ØªØ¯Ø§ Ø¢ØºØ§Ø² Ù…ÛŒâ€ŒØ´ÙˆØ¯.\")\n",
        "#-----------------------------------------------------\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch.x, batch.edge_index)\n",
        "        loss = F.cross_entropy(out, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        out = model(batch.x, batch.edge_index)\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += (pred == batch.y).sum().item()\n",
        "        total += batch.y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "\n",
        "final_epoch = 3  # ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø§Ù¾ÙˆÚ©â€ŒÙ‡Ø§\n",
        "\n",
        "for epoch in range(start_epoch, final_epoch + 1):\n",
        "    loss = train()\n",
        "    train_acc = test(train_loader)\n",
        "    val_acc = test(val_loader)\n",
        "    test_acc = test(test_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch:03d}, Loss: {loss:.4f}, \"\n",
        "          f\"Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), f\"Bettergcn_node_epoch_{epoch:03d}.pt\")\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict()\n",
        "    }, save_path)\n",
        "    print(f\"ğŸ’¾ Ù…Ø¯Ù„ Ø¯Ø± Google Drive Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: epoch {epoch}\")\n",
        "\n",
        "    # Ù¾Ø§ÛŒØ§Ù† Ø¢Ù…ÙˆØ²Ø´\n",
        "    if epoch == final_epoch:\n",
        "        print(\" \")\n",
        "\n",
        "print(\"âœ… Ø¢Ù…ÙˆØ²Ø´ Ú©Ø§Ù…Ù„ Ø´Ø¯ Ùˆ Ù…Ø¯Ù„ Ù†Ù‡Ø§ÛŒÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª.\")"
      ],
      "metadata": {
        "id": "YQ7wCFTebELK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "#  Ù…Ø¯Ù„ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡ GCN\n",
        "class ImprovedGCN(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels, bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels, bias=True)\n",
        "        self.dropout = dropout\n",
        "        self.act = nn.LeakyReLU(negative_slope=0.1)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.conv1.reset_parameters()\n",
        "        self.conv2.reset_parameters()\n",
        "        self.bn1.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = self.act(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "#  Label Smoothing Loss\n",
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    def __init__(self, eps=0.1):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        n_classes = pred.size(1)\n",
        "        log_preds = F.log_softmax(pred, dim=1)\n",
        "        target_onehot = F.one_hot(target, num_classes=n_classes).float()\n",
        "        target_smooth = target_onehot * (1 - self.eps) + self.eps / n_classes\n",
        "        loss = -(target_smooth * log_preds).sum(dim=1).mean()\n",
        "        return loss\n",
        "\n",
        "#  ØªÙ†Ø¸ÛŒÙ…Ø§Øª\n",
        "model = ImprovedGCN(num_features, hidden_channels, num_classes).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = LabelSmoothingCrossEntropy(eps=0.1)\n",
        "\n",
        "#  Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø§ Early Stopping (ØªØ§ 3 epoch Ø¨ÛŒâ€ŒØ¨Ù‡Ø¨ÙˆØ¯ â†’ ØªÙˆÙ‚Ù)\n",
        "best_val_acc = 0\n",
        "patience = 3\n",
        "wait = 0\n",
        "start_epoch = 1\n",
        "\n",
        "for epoch in range(start_epoch, start_epoch + 3):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch.x, batch.edge_index)\n",
        "        loss = criterion(out, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    model.eval()\n",
        "    def eval(loader):\n",
        "        correct, total = 0, 0\n",
        "        for batch in loader:\n",
        "            batch = batch.to(device)\n",
        "            out = model(batch.x, batch.edge_index)\n",
        "            pred = out.argmax(dim=1)\n",
        "            correct += (pred == batch.y).sum().item()\n",
        "            total += batch.y.size(0)\n",
        "        return correct / total\n",
        "\n",
        "    train_acc = eval(train_loader)\n",
        "    val_acc = eval(val_loader)\n",
        "    test_acc = eval(test_loader)\n",
        "\n",
        "    print(f\"[Epoch {epoch:02d}] Loss: {total_loss:.4f} | Train: {train_acc:.4f} | Val: {val_acc:.4f} | Test: {test_acc:.4f}\")\n",
        "    torch.save(model.state_dict(), f\"Impgcn_node_epoch_{epoch:03d}.pt\")\n",
        "\n",
        "    #  Early Stopping Logic\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        wait = 0\n",
        "        torch.save(model.state_dict(), \"/content/drive/MyDrive/improved_gcn_bestNode.pt\")\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= patience:\n",
        "            print(\" Ø¢Ù…ÙˆØ²Ø´ Ù…ØªÙˆÙ‚Ù Ø´Ø¯ (early stop).\")\n",
        "            break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "kDyelipG4Oi6",
        "outputId": "2520c860-77a5-488e-c4ee-b2bdf86b152d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-4110775579>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Epoch {epoch:02d}] Loss: {total_loss:.4f} | Train: {train_acc:.4f} | Val: {val_acc:.4f} | Test: {test_acc:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-4110775579>\u001b[0m in \u001b[0;36meval\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/loader/base.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/loader/node_loader.py\u001b[0m in \u001b[0;36mcollate_fn\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0minput_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNodeSamplerInput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_sampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_from_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_per_worker\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Execute `filter_fn` in the worker process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/sampler/neighbor_sampler.py\u001b[0m in \u001b[0;36msample_from_nodes\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNodeSamplerInput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     ) -> Union[SamplerOutput, HeteroSamplerOutput]:\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnode_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;31m# Edge-based sampling #####################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/sampler/neighbor_sampler.py\u001b[0m in \u001b[0;36mnode_sample\u001b[0;34m(inputs, sample_fn)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mseed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/sampler/neighbor_sampler.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, seed, seed_time, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m                 \u001b[0;31m# TODO (matthias) `return_edge_id` if edge features present\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                 \u001b[0;31m# TODO (matthias) Ideally, `seed` inherits dtype from `colptr`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m                 out = torch.ops.pyg.neighbor_sample(\n\u001b[0m\u001b[1;32m    283\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolptr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;31m# We save the function ptr as the `op` attribute on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# OpOverloadPacket to access it here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0;31m# TODO: use this to make a __dir__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Crach & no run"
      ],
      "metadata": {
        "id": "qtIkHvvT4kzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ù…Ù‚Ø§Ø¯ÛŒØ± Ø§Ø² Ø¯ÛŒØªØ§Ø³Øª\n",
        "num_features = data.num_node_features       # =100\n",
        "num_classes = int(data.y.max().item()) + 1  # =47\n",
        "hidden_channels = 128                       # Ù‚Ø§Ø¨Ù„ ØªÙ†Ø¸ÛŒÙ…\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Ø³Ø§Ø®Øª Ù…Ø¯Ù„\n",
        "model = GCN(num_features, hidden_channels, num_classes).to(device)\n",
        "data = data.to(device)\n",
        "train_idx = train_idx.to(device)\n",
        "val_idx = val_idx.to(device)\n",
        "test_idx = test_idx.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "# def train():\n",
        "#     model.train()\n",
        "#     optimizer.zero_grad()\n",
        "#     out = model(data)\n",
        "#     loss = F.cross_entropy(out[train_idx], data.y[train_idx])\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     return loss.item()\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch.x, batch.edge_index)\n",
        "        loss = F.cross_entropy(out, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "# @torch.no_grad()\n",
        "# def test():\n",
        "#     model.eval()\n",
        "#     out = model(data)\n",
        "#     pred = out.argmax(dim=1)\n",
        "\n",
        "#     accs = []\n",
        "#     for idx in [train_idx, val_idx, test_idx]:\n",
        "#         correct = (pred[idx] == data.y[idx]).sum().item()\n",
        "#         acc = correct / idx.shape[0]\n",
        "#         accs.append(acc)\n",
        "#     return accs\n",
        "@torch.no_grad()\n",
        "def test(loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        out = model(batch.x, batch.edge_index)\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += (pred == batch.y).sum().item()\n",
        "        total += batch.y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "for epoch in range(1, 31):  # ÙÙ‚Ø· 30 Ø¯ÙˆØ±Ù‡ Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª Ø§ÙˆÙ„ÛŒÙ‡\n",
        "    loss = train()\n",
        "    train_acc = test(train_loader)\n",
        "    val_acc = test(val_loader)\n",
        "    test_acc = test(test_loader)\n",
        "    print(f\"Epoch {epoch:03d}, Loss: {loss:.4f}, \"\n",
        "          f\"Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaeHiK4nEe5i",
        "outputId": "cf10a058-c3e9-4ca5-e171-7a083a2e0590"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 001, Loss: 1.4965, Train: 0.6650, Val: 0.6638, Test: 0.5794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GraphSAGE"
      ],
      "metadata": {
        "id": "zFBE7p6GNDoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YiaEf5ayIM2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import SAGEConv\n",
        "\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
        "\n",
        "    # def forward(self, data):\n",
        "    #     x, edge_index = data.x, data.edge_index\n",
        "    #     x = self.conv1(x, edge_index)\n",
        "    #     x = F.relu(x)\n",
        "    #     x = F.dropout(x, p=0.5, training=self.training)\n",
        "    #     x = self.conv2(x, edge_index)\n",
        "    #     return x\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.3, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "PsCUf037NBsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sage_ckpt_path = \"/content/drive/MyDrive/sage_node_last.pt\"\n",
        "\n",
        "sage_model = GraphSAGE(num_features, hidden_channels, num_classes).to(device)\n",
        "optimizer = torch.optim.Adam(sage_model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "\n",
        "start_epoch = 1\n",
        "import os\n",
        "if os.path.exists(sage_ckpt_path):\n",
        "    checkpoint = torch.load(sage_ckpt_path)\n",
        "    sage_model.load_state_dict(checkpoint['model'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    print(f\"âœ… Ø§Ø¯Ø§Ù…Ù‡ Ø¢Ù…ÙˆØ²Ø´ GraphSAGE Ø§Ø² epoch {start_epoch}\")\n",
        "else:\n",
        "    print(\"ğŸŸ¡ Ø¢Ù…ÙˆØ²Ø´ GraphSAGE Ø§Ø² Ø§Ø¨ØªØ¯Ø§ Ø´Ø±ÙˆØ¹ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\")\n",
        "\n",
        "\n",
        "def train_sage():\n",
        "    sage_model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = sage_model(batch.x, batch.edge_index)\n",
        "        loss = F.cross_entropy(out, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_sage(loader):\n",
        "    sage_model.eval()\n",
        "    correct = total = 0\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        out = sage_model(batch.x, batch.edge_index)\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += (pred == batch.y).sum().item()\n",
        "        total += batch.y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "\n",
        "sage_train_acc_list = []\n",
        "sage_val_acc_list = []\n",
        "sage_test_acc_list = []\n",
        "\n",
        "\n",
        "final_epoch = 3  # Ú©Ù„ ØªØ¹Ø¯Ø§Ø¯ Ø§Ù¾ÙˆÚ©â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø±\n",
        "\n",
        "for epoch in range(start_epoch, final_epoch + 1):\n",
        "    loss = train_sage()\n",
        "    print(\"--------\")\n",
        "    train_acc = test_sage(train_loader)\n",
        "    val_acc = test_sage(val_loader)\n",
        "    test_acc = test_sage(test_loader)\n",
        "\n",
        "    print(f\"[GraphSAGE] Epoch {epoch:03d}, Loss: {loss:.4f}, \"\n",
        "          f\"Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}\")\n",
        "\n",
        "    # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ø¯Ø± Google Drive (Ø§Ø¯Ø§Ù…Ù‡â€ŒÙ¾Ø°ÛŒØ±)\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model': sage_model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict()\n",
        "    }, sage_ckpt_path)\n",
        "\n",
        "    # Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¯Ø± Ù…Ø³ÛŒØ± Ù…Ø­Ù„ÛŒ Ø¨Ø§ Ø´Ù…Ø§Ø±Ù‡ Ø§Ù¾ÙˆÚ©\n",
        "    torch.save(sage_model.state_dict(), f\"sage_node_epoch_{epoch:03d}.pt\")\n",
        "    print(f\"ğŸ’¾ Ù…Ø¯Ù„ GraphSAGE Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: epoch {epoch}\")\n",
        "\n",
        "    # Ø§Ú¯Ø± Ø¨Ù‡ Ø¢Ø®Ø± Ø§Ù¾ÙˆÚ© Ø±Ø³ÛŒØ¯:\n",
        "    if epoch == final_epoch:\n",
        "        print(\"âœ… Ø¢Ù…ÙˆØ²Ø´ Ú©Ø§Ù…Ù„ GraphSAGE Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯.\")\n",
        "\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "WywYu6c7NIrw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "415f908b-0576-40ce-fa5e-2a2cf9085fdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Ø§Ø¯Ø§Ù…Ù‡ Ø¢Ù…ÙˆØ²Ø´ GraphSAGE Ø§Ø² epoch 4\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_sage_all_metrics(model, loader, name=\"GraphSAGE\"):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        out = model(batch.x, batch.edge_index)\n",
        "        pred = out.argmax(dim=1)\n",
        "        all_preds.append(pred.cpu())\n",
        "        all_labels.append(batch.y.cpu())\n",
        "\n",
        "    y_true = torch.cat(all_labels).numpy()\n",
        "    y_pred = torch.cat(all_preds).numpy()\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
        "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "    print(f\"\\nğŸ“Š Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ {name}:\")\n",
        "    print(f\"Accuracy      : {acc:.4f}\")\n",
        "    print(f\"F1-Score(Macro): {f1_macro:.4f}\")\n",
        "    print(f\"F1-Score(Micro): {f1_micro:.4f}\")\n",
        "    print(f\"Precision      : {precision:.4f}\")\n",
        "    print(f\"Recall         : {recall:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"Accuracy\": acc,\n",
        "        \"F1-Macro\": f1_macro,\n",
        "        \"F1-Micro\": f1_micro,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall\n",
        "    }\n",
        "\n",
        "# Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø±ÙˆÛŒ Ù…Ø¯Ù„ GraphSAGE Ùˆ Ù„ÙˆØ¯Ø± ØªØ³Øª\n",
        "sage_metrics = evaluate_sage_all_metrics(sage_model, test_loader, name=\"GraphSAGE\")\n",
        "evaluate_gcn_all_metrics(model, val_loader, name=\"SAGE-Validation\")\n",
        "evaluate_gcn_all_metrics(model, train_loader, name=\"SAGE-Train\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5oTM86NmGK_",
        "outputId": "3f4b9b66-1fee-4325-e96e-901f0b5e2e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Š Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ GraphSAGE:\n",
            "Accuracy      : 0.5690\n",
            "F1-Score(Macro): 0.2060\n",
            "F1-Score(Micro): 0.5690\n",
            "Precision      : 0.3068\n",
            "Recall         : 0.2011\n",
            "\n",
            "ğŸ“Š Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ SAGE-Validation:\n",
            "Accuracy    : 0.6023\n",
            "F1-Score(Macro): 0.1812\n",
            "F1-Score(Micro): 0.6023\n",
            "Precision   : 0.2893\n",
            "Recall      : 0.1614\n",
            "\n",
            "ğŸ“Š Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ SAGE-Train:\n",
            "Accuracy    : 0.6019\n",
            "F1-Score(Macro): 0.1806\n",
            "F1-Score(Micro): 0.6019\n",
            "Precision   : 0.2858\n",
            "Recall      : 0.1611\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Accuracy': 0.6019413583103213,\n",
              " 'F1-Macro': 0.1806062954188998,\n",
              " 'F1-Micro': 0.6019413583103213,\n",
              " 'Precision': 0.28575297160296087,\n",
              " 'Recall': 0.1611162531910095}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eval"
      ],
      "metadata": {
        "id": "YBt5XEnkN6ge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "@torch.no_grad()\n",
        "def collect_predictions(model, loader):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        out = model(batch.x, batch.edge_index)\n",
        "        pred = out.argmax(dim=1)\n",
        "        y_true.append(batch.y.cpu())\n",
        "        y_pred.append(pred.cpu())\n",
        "    return torch.cat(y_true), torch.cat(y_pred)\n",
        "\n",
        "# Ú¯Ø±ÙØªÙ† Ø®Ø±ÙˆØ¬ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§\n",
        "y_true_gcn, y_pred_gcn = collect_predictions(model, test_loader)\n",
        "y_true_sage, y_pred_sage = collect_predictions(sage_model, test_loader)\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"F1-Macro\": f1_score(y_true, y_pred, average='macro'),\n",
        "        \"F1-Micro\": f1_score(y_true, y_pred, average='micro'),\n",
        "        \"Precision\": precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
        "        \"Recall\": recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    }\n",
        "\n",
        "# Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§\n",
        "gcn_metrics = compute_metrics(y_true_gcn, y_pred_gcn)\n",
        "sage_metrics = compute_metrics(y_true_sage, y_pred_sage)\n",
        "\n",
        "# Ø³Ø§Ø®Øª Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ\n",
        "df = pd.DataFrame([gcn_metrics, sage_metrics], index=[\"GCN\", \"GraphSAGE\"])\n",
        "\n",
        "# Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ\n",
        "df.plot(kind='bar', figsize=(10, 6), colormap='tab10')\n",
        "plt.title(\"ğŸ“Š Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ GCN Ùˆ GraphSAGE\")\n",
        "plt.ylabel(\"Ù…Ù‚Ø¯Ø§Ø±\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Ù†Ù…Ø§ÛŒØ´ Ø¬Ø¯ÙˆÙ„ Ù†Ù‡Ø§ÛŒÛŒ\n",
        "print(\" Ø¬Ø¯ÙˆÙ„ Ù†Ù‡Ø§ÛŒÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡:\")\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "mKz9TMNHnBFd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "outputId": "3f47ba7e-d86f-41f7-dca6-9e1f47429c6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-3224154149>:45: UserWarning: Glyph 128202 (\\N{BAR CHART}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 128202 (\\N{BAR CHART}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Ø¬Ø¯ÙˆÙ„ Ù†Ù‡Ø§ÛŒÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡:\n",
            "           Accuracy  F1-Macro  F1-Micro  Precision    Recall\n",
            "GCN        0.562116  0.196033  0.562116   0.287407  0.190544\n",
            "GraphSAGE  0.568981  0.205922  0.568981   0.306326  0.200950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra Point"
      ],
      "metadata": {
        "id": "-wjpHZLW5H1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Y-gMnLWtP1VT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "907cf541-46ee-41bf-a6c6-92fd4fce8ac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  GCN"
      ],
      "metadata": {
        "id": "f_2jxfprSFNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- Imports\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "from torch_geometric.data import Data\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "import numpy as np\n",
        "\n",
        "# ---------------------- Load Dataset (no unpacking)\n",
        "dataset = PygNodePropPredDataset(name='ogbn-products', root='/tmp/ogb')\n",
        "data = dataset[0]\n",
        "data.y = data.y.squeeze()\n",
        "\n",
        "# ---------------------- Preprocess\n",
        "transform = RandomLinkSplit(\n",
        "    is_undirected=True,\n",
        "    split_labels=True,\n",
        "    add_negative_train_samples=False,\n",
        "    num_val=0.05,\n",
        "    num_test=0.2\n",
        ")\n",
        "train_data, val_data, test_data = transform(data)\n",
        "\n",
        "# ---------------------- Encoder\n",
        "class GCNEncoder(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, 64)\n",
        "        self.conv2 = GCNConv(64, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# ---------------------- Decoder\n",
        "def decode(z, edge_index):\n",
        "    return (z[edge_index[0]] * z[edge_index[1]]).sum(dim=1)\n",
        "\n",
        "# ---------------------- Loss\n",
        "def compute_loss(z, pos_edge_index, num_nodes, batch_size=100000):\n",
        "    pos_score = decode(z, pos_edge_index)\n",
        "    pos_loss = -F.logsigmoid(pos_score).mean()\n",
        "\n",
        "    neg_edge_index = torch.randint(0, num_nodes, pos_edge_index.shape, device=z.device)\n",
        "    neg_score = decode(z, neg_edge_index)\n",
        "    neg_loss = -F.logsigmoid(-neg_score).mean()\n",
        "    return pos_loss + neg_loss\n",
        "\n",
        "print(\"train time:/n\")\n",
        "# ---------------------- Setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GCNEncoder(data.num_node_features, 64).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "x = train_data.x.to(device)\n",
        "train_edge_index = train_data.edge_index.to(device)\n",
        "\n",
        "# ---------------------- Train\n",
        "for epoch in range(1, 4):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    z = model(x, train_edge_index)\n",
        "    loss = compute_loss(z, train_data.pos_edge_label_index.to(device), x.size(0))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"[EdgePred] Epoch {epoch:02d} | Loss: {loss:.4f}\")\n",
        "\n",
        "# ---------------------- Evaluation\n",
        "@torch.no_grad()\n",
        "def evaluate(model, x, edge_index, pos_edge_index, neg_edge_index):\n",
        "    model.eval()\n",
        "    z = model(x, edge_index)\n",
        "    pos_score = torch.sigmoid(decode(z, pos_edge_index)).cpu().numpy()\n",
        "    neg_score = torch.sigmoid(decode(z, neg_edge_index)).cpu().numpy()\n",
        "    y_true = np.hstack([np.ones(pos_score.shape[0]), np.zeros(neg_score.shape[0])])\n",
        "    y_scores = np.hstack([pos_score, neg_score])\n",
        "    auc = roc_auc_score(y_true, y_scores)\n",
        "    ap = average_precision_score(y_true, y_scores)\n",
        "    return auc, ap\n",
        "\n",
        "x = data.x.to(device)\n",
        "full_edge_index = data.edge_index.to(device)\n",
        "val_auc, val_ap = evaluate(model, x, full_edge_index, val_data.pos_edge_label_index.to(device), val_data.neg_edge_label_index.to(device))\n",
        "test_auc, test_ap = evaluate(model, x, full_edge_index, test_data.pos_edge_label_index.to(device), test_data.neg_edge_label_index.to(device))\n",
        "\n",
        "print(f\"\\nğŸ“Š Validation AUC: {val_auc:.4f}, AP: {val_ap:.4f}\")\n",
        "print(f\"ğŸ“Š Test AUC     : {test_auc:.4f}, AP: {test_ap:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHiYmBKj5G46",
        "outputId": "cf24228f-14b3-4eb6-d5f1-2dbf387967cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train time:/n\n",
            "[EdgePred] Epoch 01 | Loss: 2.0079\n",
            "[EdgePred] Epoch 02 | Loss: 1.1356\n",
            "[EdgePred] Epoch 03 | Loss: 1.0295\n",
            "\n",
            "ğŸ“Š Validation AUC: 0.9550, AP: 0.9512\n",
            "ğŸ“Š Test AUC     : 0.9550, AP: 0.9512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"gcn_edge_model.pt\")\n"
      ],
      "metadata": {
        "id": "y071E94wB7dB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCNEncoder(data.num_node_features, 64)\n",
        "model.load_state_dict(torch.load(\"gcn_edge_model.pt\"))\n",
        "model = model.to(device)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HeKmA4kCsGf",
        "outputId": "122ee206-2be5-4af4-f1c5-ce4c5d5f8da2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GCNEncoder(\n",
              "  (conv1): GCNConv(100, 64)\n",
              "  (conv2): GCNConv(64, 64)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# ---------------------- Load Dataset\n",
        "dataset = PygNodePropPredDataset(name='ogbn-products', root='/tmp/ogb')\n",
        "data = dataset[0]\n",
        "data.y = data.y.squeeze()\n",
        "\n",
        "# ---------------------- Preprocess\n",
        "transform = RandomLinkSplit(\n",
        "    is_undirected=True,\n",
        "    split_labels=True,\n",
        "    add_negative_train_samples=False,\n",
        "    num_val=0.05,\n",
        "    num_test=0.2\n",
        ")\n",
        "train_data, val_data, test_data = transform(data)\n",
        "\n",
        "# ---------------------- Model Definition\n",
        "class GCNEncoder(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, 64)\n",
        "        self.conv2 = GCNConv(64, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "def decode(z, edge_index):\n",
        "    return (z[edge_index[0]] * z[edge_index[1]]).sum(dim=1)\n",
        "\n",
        "def compute_loss(z, pos_edge_index, num_nodes, batch_size=100000):\n",
        "    pos_score = decode(z, pos_edge_index)\n",
        "    pos_loss = -F.logsigmoid(pos_score).mean()\n",
        "\n",
        "    neg_edge_index = torch.randint(0, num_nodes, pos_edge_index.shape, device=z.device)\n",
        "    neg_score = decode(z, neg_edge_index)\n",
        "    neg_loss = -F.logsigmoid(-neg_score).mean()\n",
        "    return pos_loss + neg_loss\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, x, edge_index, pos_edge_index, neg_edge_index):\n",
        "    model.eval()\n",
        "    z = model(x, edge_index)\n",
        "    pos_score = torch.sigmoid(decode(z, pos_edge_index)).cpu().numpy()\n",
        "    neg_score = torch.sigmoid(decode(z, neg_edge_index)).cpu().numpy()\n",
        "    y_true = np.hstack([np.ones(pos_score.shape[0]), np.zeros(neg_score.shape[0])])\n",
        "    y_scores = np.hstack([pos_score, neg_score])\n",
        "    auc = roc_auc_score(y_true, y_scores)\n",
        "    ap = average_precision_score(y_true, y_scores)\n",
        "    return auc, ap\n",
        "\n",
        "# ---------------------- Training Setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GCNEncoder(data.num_node_features, 64).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "save_path = \"/content/drive/MyDrive/gcn_edge_model.pt\"\n",
        "\n",
        "x = train_data.x.to(device)\n",
        "train_edge_index = train_data.edge_index.to(device)\n",
        "\n",
        "# ---------------------- Load if exists\n",
        "if os.path.exists(save_path):\n",
        "    model.load_state_dict(torch.load(save_path))  # ÙÙ‚Ø· state_dict Ù…Ø¯Ù„ Ø±Ø§ Ù„ÙˆØ¯ Ú©Ù†\n",
        "    print(\"âœ… ÙÙ‚Ø· Ù…Ø¯Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯ (optimizer Ø°Ø®ÛŒØ±Ù‡ Ù†Ø´Ø¯Ù‡ Ø¨ÙˆØ¯). Ø¢Ù…ÙˆØ²Ø´ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ù†ÛŒØ§Ø² Ù†ÛŒØ³Øª.\")\n",
        "else:\n",
        "    print(\"ğŸŸ¡ Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡â€ŒØ´Ø¯Ù‡ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø¢Ù…ÙˆØ²Ø´ Ø¢ØºØ§Ø² Ù…ÛŒâ€ŒØ´ÙˆØ¯.\")\n",
        "    for epoch in range(1, 4):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        z = model(x, train_edge_index)\n",
        "        loss = compute_loss(z, train_data.pos_edge_label_index.to(device), x.size(0))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"[EdgePred] Epoch {epoch:02d} | Loss: {loss:.4f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(\"ğŸ’¾ ÙÙ‚Ø· Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯ (Ø¨Ø¯ÙˆÙ† optimizer).\")\n",
        "\n",
        "    torch.save({\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict()\n",
        "    }, save_path)\n",
        "    print(\"ğŸ’¾ Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\")\n",
        "\n",
        "# ---------------------- Evaluation\n",
        "x = data.x.to(device)\n",
        "full_edge_index = data.edge_index.to(device)\n",
        "# val_auc, val_ap = evaluate(model, x, full_edge_index,\n",
        "#                            val_data.pos_edge_label_index.to(device),\n",
        "#                            val_data.neg_edge_label_index.to(device))\n",
        "\n",
        "# test_auc, test_ap = evaluate(model, x, full_edge_index,\n",
        "#                              test_data.pos_edge_label_index.to(device),\n",
        "#                              test_data.neg_edge_label_index.to(device))\n",
        "\n",
        "# print(f\"\\nğŸ“Š Validation AUC: {val_auc:.4f}, AP: {val_ap:.4f}\")\n",
        "# print(f\"ğŸ“Š Test AUC     : {test_auc:.4f}, AP: {test_ap:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-KLyMvtJJpC",
        "outputId": "25691c4c-1fca-433b-83b9-fbc5e835f4bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ÙÙ‚Ø· Ù…Ø¯Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯ (optimizer Ø°Ø®ÛŒØ±Ù‡ Ù†Ø´Ø¯Ù‡ Ø¨ÙˆØ¯). Ø¢Ù…ÙˆØ²Ø´ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ù†ÛŒØ§Ø² Ù†ÛŒØ³Øª.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SAGE"
      ],
      "metadata": {
        "id": "GaoedU5eSJkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- Imports\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# ---------------------- Load Dataset\n",
        "dataset = PygNodePropPredDataset(name='ogbn-products', root='/tmp/ogb')\n",
        "data = dataset[0]\n",
        "data.y = data.y.squeeze()\n",
        "\n",
        "# ---------------------- Preprocess for Link Prediction\n",
        "transform = RandomLinkSplit(\n",
        "    is_undirected=True,\n",
        "    split_labels=True,\n",
        "    add_negative_train_samples=False,\n",
        "    num_val=0.05,\n",
        "    num_test=0.2\n",
        ")\n",
        "train_data, val_data, test_data = transform(data)\n",
        "\n",
        "# ---------------------- GraphSAGE Encoder\n",
        "class SAGEEncoder(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, 64)\n",
        "        self.conv2 = SAGEConv(64, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# ---------------------- Decoder\n",
        "def decode(z, edge_index):\n",
        "    return (z[edge_index[0]] * z[edge_index[1]]).sum(dim=1)\n",
        "\n",
        "# ---------------------- Loss Function\n",
        "def compute_loss(z, pos_edge_index, num_nodes):\n",
        "    pos_score = decode(z, pos_edge_index)\n",
        "    pos_loss = -F.logsigmoid(pos_score).mean()\n",
        "\n",
        "    neg_edge_index = torch.randint(0, num_nodes, pos_edge_index.shape, device=z.device)\n",
        "    neg_score = decode(z, neg_edge_index)\n",
        "    neg_loss = -F.logsigmoid(-neg_score).mean()\n",
        "\n",
        "    return pos_loss + neg_loss\n",
        "\n",
        "# ---------------------- Setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = SAGEEncoder(data.num_node_features, 64).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "x = train_data.x.to(device)\n",
        "train_edge_index = train_data.edge_index.to(device)\n",
        "\n",
        "# ---------------------- Checkpoint Path\n",
        "ckpt_path = \"/content/drive/MyDrive/sage_edge_model.pt\"\n",
        "start_epoch = 1\n",
        "\n",
        "# ---------------------- Resume Training if Checkpoint Exists\n",
        "if os.path.exists(ckpt_path):\n",
        "    try:\n",
        "        checkpoint = torch.load(ckpt_path)\n",
        "        model.load_state_dict(checkpoint['model'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        print(f\"âœ… Ù…Ø¯Ù„ Ù…ÙˆØ¬ÙˆØ¯ ÛŒØ§ÙØª Ø´Ø¯. Ø§Ø¯Ø§Ù…Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø§Ø² epoch {start_epoch}\")\n",
        "    except:\n",
        "        print(\"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„. Ø¢Ù…ÙˆØ²Ø´ Ø§Ø² Ø§Ø¨ØªØ¯Ø§ Ø¢ØºØ§Ø² Ù…ÛŒâ€ŒØ´ÙˆØ¯.\")\n",
        "else:\n",
        "    print(\"ğŸŸ¡ ÙØ§ÛŒÙ„ Ø°Ø®ÛŒØ±Ù‡â€ŒØ´Ø¯Ù‡ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø¢Ù…ÙˆØ²Ø´ Ø§Ø² Ø§Ø¨ØªØ¯Ø§ Ø´Ø±ÙˆØ¹ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\")\n",
        "\n",
        "# ---------------------- Training\n",
        "for epoch in range(start_epoch, start_epoch + 3):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    z = model(x, train_edge_index)\n",
        "    loss = compute_loss(z, train_data.pos_edge_label_index.to(device), x.size(0))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"[EdgePred-SAGE] Epoch {epoch:02d} | Loss: {loss:.4f}\")\n",
        "\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict()\n",
        "    }, ckpt_path)\n",
        "    print(f\"ğŸ’¾ Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: epoch {epoch}\")\n",
        "\n",
        "# ---------------------- Evaluation\n",
        "@torch.no_grad()\n",
        "def evaluate(model, x, edge_index, pos_edge_index, neg_edge_index):\n",
        "    model.eval()\n",
        "    z = model(x, edge_index)\n",
        "    pos_score = torch.sigmoid(decode(z, pos_edge_index)).cpu().numpy()\n",
        "    neg_score = torch.sigmoid(decode(z, neg_edge_index)).cpu().numpy()\n",
        "    y_true = np.hstack([np.ones(pos_score.shape[0]), np.zeros(neg_score.shape[0])])\n",
        "    y_scores = np.hstack([pos_score, neg_score])\n",
        "    auc = roc_auc_score(y_true, y_scores)\n",
        "    ap = average_precision_score(y_true, y_scores)\n",
        "    return auc, ap\n",
        "\n",
        "x = data.x.to(device)\n",
        "full_edge_index = data.edge_index.to(device)\n",
        "\n",
        "val_auc, val_ap = evaluate(model, x, full_edge_index,\n",
        "                           val_data.pos_edge_label_index.to(device),\n",
        "                           val_data.neg_edge_label_index.to(device))\n",
        "\n",
        "test_auc, test_ap = evaluate(model, x, full_edge_index,\n",
        "                             test_data.pos_edge_label_index.to(device),\n",
        "                             test_data.neg_edge_label_index.to(device))\n",
        "\n",
        "print(f\"\\nğŸ“Š Validation AUC (SAGE): {val_auc:.4f}, AP: {val_ap:.4f}\")\n",
        "print(f\"ğŸ“Š Test AUC (SAGE)     : {test_auc:.4f}, AP: {test_ap:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GjOzvgPOl8X",
        "outputId": "57c938b0-5d87-4b15-8a06-66a366a5a3f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸŸ¡ ÙØ§ÛŒÙ„ Ø°Ø®ÛŒØ±Ù‡â€ŒØ´Ø¯Ù‡ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø¢Ù…ÙˆØ²Ø´ Ø§Ø² Ø§Ø¨ØªØ¯Ø§ Ø´Ø±ÙˆØ¹ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n",
            "[EdgePred-SAGE] Epoch 01 | Loss: 2.1617\n",
            "ğŸ’¾ Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: epoch 1\n",
            "[EdgePred-SAGE] Epoch 02 | Loss: 1.7275\n",
            "ğŸ’¾ Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: epoch 2\n",
            "[EdgePred-SAGE] Epoch 03 | Loss: 1.4842\n",
            "ğŸ’¾ Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: epoch 3\n",
            "\n",
            "ğŸ“Š Validation AUC (SAGE): 0.7555, AP: 0.7194\n",
            "ğŸ“Š Test AUC (SAGE)     : 0.7556, AP: 0.7194\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2xuk64yddTo",
        "outputId": "e9139aae-a149-46b2-a537-4980027b8405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, SAGEConv\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ---------------- Encoder Definitions\n",
        "class GCNEncoder(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, 64)\n",
        "        self.conv2 = GCNConv(64, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        return self.conv2(x, edge_index)\n",
        "\n",
        "class SAGEEncoder(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, 64)\n",
        "        self.conv2 = SAGEConv(64, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        return self.conv2(x, edge_index)\n",
        "\n",
        "# ---------------- Decoder\n",
        "def decode(z, edge_index):\n",
        "    return (z[edge_index[0]] * z[edge_index[1]]).sum(dim=1)\n",
        "\n",
        "# ---------------- Evaluation\n",
        "@torch.no_grad()\n",
        "def evaluate(model, x, edge_index, pos_edge_index, neg_edge_index):\n",
        "    model.eval()\n",
        "    z = model(x, edge_index)\n",
        "    pos_score = torch.sigmoid(decode(z, pos_edge_index)).cpu().numpy()\n",
        "    neg_score = torch.sigmoid(decode(z, neg_edge_index)).cpu().numpy()\n",
        "    y_true = np.hstack([np.ones(len(pos_score)), np.zeros(len(neg_score))])\n",
        "    y_scores = np.hstack([pos_score, neg_score])\n",
        "    auc = roc_auc_score(y_true, y_scores)\n",
        "    ap = average_precision_score(y_true, y_scores)\n",
        "    return auc, ap\n",
        "\n",
        "# ---------------- Dataset Preparation\n",
        "dataset = PygNodePropPredDataset(name='ogbn-products', root='/tmp/ogb')\n",
        "data = dataset[0]\n",
        "data.y = data.y.squeeze()\n",
        "transform = RandomLinkSplit(is_undirected=True, split_labels=True, add_negative_train_samples=False, num_val=0.05, num_test=0.2)\n",
        "train_data, val_data, test_data = transform(data)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "x = data.x.to(device)\n",
        "full_edge_index = data.edge_index.to(device)\n",
        "\n",
        "print(\"Load Models:\")\n",
        "# ---------------- Load and Evaluate GCN\n",
        "gcn_model = GCNEncoder(data.num_node_features, 64).to(device)\n",
        "gcn_model.load_state_dict(torch.load(\"/content/drive/MyDrive/gcn_edge_model.pt\"))\n",
        "\n",
        "gcn_val_auc, gcn_val_ap = evaluate(gcn_model, x, full_edge_index,\n",
        "                                   val_data.pos_edge_label_index.to(device),\n",
        "                                   val_data.neg_edge_label_index.to(device))\n",
        "gcn_test_auc, gcn_test_ap = evaluate(gcn_model, x, full_edge_index,\n",
        "                                     test_data.pos_edge_label_index.to(device),\n",
        "                                     test_data.neg_edge_label_index.to(device))\n",
        "\n",
        "# ---------------- Load and Evaluate SAGE\n",
        "sage_model = SAGEEncoder(data.num_node_features, 64).to(device)\n",
        "sage_ckpt = torch.load(\"/content/drive/MyDrive/sage_edge_model.pt\")\n",
        "sage_model.load_state_dict(sage_ckpt['model'])\n",
        "\n",
        "sage_val_auc, sage_val_ap = evaluate(sage_model, x, full_edge_index,\n",
        "                                     val_data.pos_edge_label_index.to(device),\n",
        "                                     val_data.neg_edge_label_index.to(device))\n",
        "sage_test_auc, sage_test_ap = evaluate(sage_model, x, full_edge_index,\n",
        "                                       test_data.pos_edge_label_index.to(device),\n",
        "                                       test_data.neg_edge_label_index.to(device))\n",
        "\n",
        "# ---------------- Print Results\n",
        "print(\"ğŸ“Š GCN Validation AUC:\", gcn_val_auc, \"AP:\", gcn_val_ap)\n",
        "print(\"ğŸ“Š GCN Test AUC:\", gcn_test_auc, \"AP:\", gcn_test_ap)\n",
        "print(\"ğŸ“Š SAGE Validation AUC:\", sage_val_auc, \"AP:\", sage_val_ap)\n",
        "print(\"ğŸ“Š SAGE Test AUC:\", sage_test_auc, \"AP:\", sage_test_ap)\n",
        "\n",
        "# ---------------- Plot Comparison\n",
        "labels = ['Validation AUC', 'Validation AP', 'Test AUC', 'Test AP']\n",
        "gcn_scores = [gcn_val_auc, gcn_val_ap, gcn_test_auc, gcn_test_ap]\n",
        "sage_scores = [sage_val_auc, sage_val_ap, sage_test_auc, sage_test_ap]\n",
        "\n",
        "x_pos = np.arange(len(labels))\n",
        "bar_width = 0.35\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(x_pos - bar_width/2, gcn_scores, width=bar_width, label='GCN')\n",
        "plt.bar(x_pos + bar_width/2, sage_scores, width=bar_width, label='GraphSAGE')\n",
        "plt.xticks(x_pos, labels)\n",
        "plt.ylim(0, 1)\n",
        "plt.ylabel('Score')\n",
        "plt.title('GCN vs GraphSAGE on Edge Prediction (ogbn-products)')\n",
        "plt.legend()\n",
        "plt.grid(True, axis='y')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4ULyfXaZ5_Kc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798
        },
        "outputId": "bed5e203-16de-4169-9d7a-9a6db394fc6e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This will download 1.38GB. Will you proceed? (y/N)\n",
            "y\n",
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/products.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 1.38 GB: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1414/1414 [00:26<00:00, 53.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /tmp/ogb/products.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 3057.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load Models:\n",
            "ğŸ“Š GCN Validation AUC: 0.9549023804232156 AP: 0.9511350289596431\n",
            "ğŸ“Š GCN Test AUC: 0.9549842582604046 AP: 0.9511892404959673\n",
            "ğŸ“Š SAGE Validation AUC: 0.7553068539863684 AP: 0.7190979006726714\n",
            "ğŸ“Š SAGE Test AUC: 0.7556644094278308 AP: 0.7194285217680413\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##-----------------------------------------"
      ],
      "metadata": {
        "id": "r0BKLttPEgh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "# !pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
        "# !pip install ogb"
      ],
      "metadata": {
        "id": "G1LGi8v0-CyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "jZLUBMLbcE85",
        "outputId": "ee03de6b-4fe2-4868-a7fc-750b44cd38be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This will download 1.38GB. Will you proceed? (y/N)\n",
            "y\n",
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/products.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 1.38 GB: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1414/1414 [00:32<00:00, 44.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/products.zip\n",
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n",
            "\n",
            "Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ GraphSAGE:\n",
            "Epoch 10/100, Loss: 0.8670, Val Acc: 0.8166, Val F1: 0.8051\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-1739035407>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m# 4. Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nØ¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ GraphSAGE:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m sage_model, sage_loss, sage_val_acc, sage_val_f1 = train_model(\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0mGraphSAGE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-1739035407>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, data, epochs)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-1739035407>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/nn/conv/sage_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, size)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# propagate_type: (x: OptPairTensor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin_l\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m                         \u001b[0maggr_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0maggr_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aggregate_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, inputs, index, ptr, dim_size)\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0;32mas\u001b[0m \u001b[0mspecified\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mby\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0maggr\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0margument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \"\"\"\n\u001b[0;32m--> 608\u001b[0;31m         return self.aggr_module(inputs, index, ptr=ptr, dim_size=dim_size,\n\u001b[0m\u001b[1;32m    609\u001b[0m                                 dim=self.node_dim)\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/nn/aggr/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, index, ptr, dim_size, dim, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIndexError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/nn/aggr/basic.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, index, ptr, dim_size, dim)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mptr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 dim: int = -2) -> Tensor:\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/nn/aggr/base.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, x, index, ptr, dim_size, dim, reduce)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     def to_dense_batch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/scatter.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_add_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from ogb.nodeproppred import NodePropPredDataset\n",
        "from torch_geometric.nn import GCNConv, SAGEConv\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# 1. Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
        "dataset = NodePropPredDataset(name='ogbn-products')\n",
        "split_idx = dataset.get_idx_split()\n",
        "\n",
        "graph, labels = dataset[0]\n",
        "edge_index = torch.tensor(graph['edge_index'], dtype=torch.long)\n",
        "x = torch.tensor(graph['node_feat'], dtype=torch.float)\n",
        "y = torch.tensor(labels, dtype=torch.long).squeeze()\n",
        "\n",
        "# Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒØŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ùˆ ØªØ³Øª\n",
        "data = Data(x=x, edge_index=edge_index, y=y)\n",
        "data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
        "data.val_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
        "data.test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
        "\n",
        "data.train_mask[split_idx[\"train\"]] = True\n",
        "data.val_mask[split_idx[\"valid\"]] = True\n",
        "data.test_mask[split_idx[\"test\"]] = True\n",
        "\n",
        "# 2. ØªØ¹Ø±ÛŒÙ Ù…Ø¯Ù„â€ŒÙ‡Ø§\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "        self.dropout = torch.nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
        "        self.dropout = torch.nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# 3. ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ù…ÙˆØ²Ø´\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "data = data.to(device)\n",
        "\n",
        "def train_model(model, data, epochs=100):\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    train_losses, val_accs, val_f1s = [], [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        out = model(data)\n",
        "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "\n",
        "            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¯Ù‚Øª Ùˆ F1 Ø¨Ø±Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ\n",
        "            val_acc = accuracy_score(data.y[data.val_mask].cpu(),\n",
        "                                   pred[data.val_mask].cpu())\n",
        "            val_f1 = f1_score(data.y[data.val_mask].cpu(),\n",
        "                             pred[data.val_mask].cpu(), average='weighted')\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "        val_accs.append(val_acc)\n",
        "        val_f1s.append(val_f1)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, '\n",
        "                  f'Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}')\n",
        "\n",
        "    return model, train_losses, val_accs, val_f1s\n",
        "\n",
        "# 4. Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§\n",
        "print(\"\\nØ¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ GraphSAGE:\")\n",
        "sage_model, sage_loss, sage_val_acc, sage_val_f1 = train_model(\n",
        "    GraphSAGE(data.num_features, 256, dataset.num_classes),\n",
        "    data\n",
        ")\n",
        "\n",
        "print(\"\\nØ¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ GCN:\")\n",
        "gcn_model, gcn_loss, gcn_val_acc, gcn_val_f1 = train_model(\n",
        "    GCN(data.num_features, 256, dataset.num_classes),\n",
        "    data\n",
        ")\n",
        "\n",
        "# 5. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ\n",
        "def evaluate_model(model, data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data)\n",
        "        pred = out.argmax(dim=1)\n",
        "\n",
        "        train_acc = accuracy_score(data.y[data.train_mask].cpu(),\n",
        "                                 pred[data.train_mask].cpu())\n",
        "        val_acc = accuracy_score(data.y[data.val_mask].cpu(),\n",
        "                               pred[data.val_mask].cpu())\n",
        "        test_acc = accuracy_score(data.y[data.test_mask].cpu(),\n",
        "                                pred[data.test_mask].cpu())\n",
        "\n",
        "        test_f1 = f1_score(data.y[data.test_mask].cpu(),\n",
        "                         pred[data.test_mask].cpu(), average='weighted')\n",
        "\n",
        "        return train_acc, val_acc, test_acc, test_f1\n",
        "\n",
        "print(\"\\nØ§Ø±Ø²ÛŒØ§Ø¨ÛŒ GraphSAGE:\")\n",
        "sage_train_acc, sage_val_acc, sage_test_acc, sage_test_f1 = evaluate_model(sage_model, data)\n",
        "print(f\"Train Acc: {sage_train_acc:.4f}, Val Acc: {sage_val_acc:.4f}, \"\n",
        "      f\"Test Acc: {sage_test_acc:.4f}, Test F1: {sage_test_f1:.4f}\")\n",
        "\n",
        "print(\"\\nØ§Ø±Ø²ÛŒØ§Ø¨ÛŒ GCN:\")\n",
        "gcn_train_acc, gcn_val_acc, gcn_test_acc, gcn_test_f1 = evaluate_model(gcn_model, data)\n",
        "print(f\"Train Acc: {gcn_train_acc:.4f}, Val Acc: {gcn_val_acc:.4f}, \"\n",
        "      f\"Test Acc: {gcn_test_acc:.4f}, Test F1: {gcn_test_f1:.4f}\")\n",
        "\n",
        "# 6. Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Ù†Ù…ÙˆØ¯Ø§Ø± Ø®Ø·Ø§\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(sage_loss, label='GraphSAGE')\n",
        "plt.plot(gcn_loss, label='GCN')\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Ù†Ù…ÙˆØ¯Ø§Ø± Ø¯Ù‚Øª Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(sage_val_acc, label='GraphSAGE')\n",
        "plt.plot(gcn_val_acc, label='GCN')\n",
        "plt.title('Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Ù…Ù‚Ø§ÛŒØ³Ù‡ F1\n",
        "plt.subplot(2, 2, 3)\n",
        "models = ['GraphSAGE', 'GCN']\n",
        "test_f1 = [sage_test_f1, gcn_test_f1]\n",
        "plt.bar(models, test_f1, color=['blue', 'orange'])\n",
        "plt.title('Test F1-Score Comparison')\n",
        "plt.ylabel('F1-Score')\n",
        "\n",
        "# Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¯Ù‚Øª ØªØ³Øª\n",
        "plt.subplot(2, 2, 4)\n",
        "test_acc = [sage_test_acc, gcn_test_acc]\n",
        "plt.bar(models, test_acc, color=['blue', 'orange'])\n",
        "plt.title('Test Accuracy Comparison')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('results_comparison.png')\n",
        "plt.show()\n",
        "\n",
        "# 7. Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ø¶Ø§ÙÛŒ: Edge Prediction\n",
        "class EdgePredictor(torch.nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(EdgePredictor, self).__init__()\n",
        "        self.lin1 = torch.nn.Linear(2 * in_channels, 128)\n",
        "        self.lin2 = torch.nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, z, edge_index):\n",
        "        src, dst = edge_index\n",
        "        x = torch.cat([z[src], z[dst]], dim=1)\n",
        "        x = F.relu(self.lin1(x))\n",
        "        return torch.sigmoid(self.lin2(x)).squeeze()\n",
        "\n",
        "def get_embeddings(model, data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ embeddings Ø§Ø² Ù„Ø§ÛŒÙ‡ Ø§ÙˆÙ„\n",
        "        embeddings = model.conv1(data.x, data.edge_index)\n",
        "        embeddings = F.relu(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "print(\"\\nØ¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Edge Prediction Ø¨Ø§ GraphSAGE:\")\n",
        "sage_embeddings = get_embeddings(sage_model, data)\n",
        "edge_model = EdgePredictor(256).to(device)\n",
        "optimizer = torch.optim.Adam(edge_model.parameters(), lr=0.01)\n",
        "\n",
        "# Ù†Ù…ÙˆÙ†Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø² ÛŒØ§Ù„Ù‡Ø§ÛŒ Ù…Ù†ÙÛŒ\n",
        "def negative_sampling(edge_index, num_nodes, num_neg_samples=None):\n",
        "    if num_neg_samples is None:\n",
        "        num_neg_samples = edge_index.size(1)\n",
        "\n",
        "    neg_edge_index = torch.randint(0, num_nodes, (2, num_neg_samples), device=device)\n",
        "    return neg_edge_index\n",
        "\n",
        "for epoch in range(50):\n",
        "    edge_model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø±Ø§ÛŒ ÛŒØ§Ù„Ù‡Ø§ÛŒ Ù…Ø«Ø¨Øª\n",
        "    pos_pred = edge_model(sage_embeddings, data.edge_index)\n",
        "    pos_loss = F.binary_cross_entropy(pos_pred, torch.ones_like(pos_pred))\n",
        "\n",
        "    # Ù†Ù…ÙˆÙ†Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ùˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø±Ø§ÛŒ ÛŒØ§Ù„Ù‡Ø§ÛŒ Ù…Ù†ÙÛŒ\n",
        "    neg_edge_index = negative_sampling(data.edge_index, data.num_nodes, num_neg_samples=data.edge_index.size(1))\n",
        "    neg_pred = edge_model(sage_embeddings, neg_edge_index)\n",
        "    neg_loss = F.binary_cross_entropy(neg_pred, torch.zeros_like(neg_pred))\n",
        "\n",
        "    loss = pos_loss + neg_loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch {epoch+1}/50, Loss: {loss.item():.4f}')\n",
        "\n",
        "# Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Edge Prediction\n",
        "edge_model.eval()\n",
        "with torch.no_grad():\n",
        "    pos_pred = edge_model(sage_embeddings, data.edge_index)\n",
        "    neg_edge_index = negative_sampling(data.edge_index, data.num_nodes, num_neg_samples=100000)\n",
        "    neg_pred = edge_model(sage_embeddings, neg_edge_index)\n",
        "\n",
        "    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¯Ù‚Øª\n",
        "    pos_acc = (pos_pred > 0.5).float().mean()\n",
        "    neg_acc = (neg_pred < 0.5).float().mean()\n",
        "    overall_acc = (pos_acc * pos_pred.size(0) + neg_acc * neg_pred.size(0)) / (pos_pred.size(0) + neg_pred.size(0))\n",
        "\n",
        "    print(f\"\\nÙ†ØªØ§ÛŒØ¬ Edge Prediction:\")\n",
        "    print(f\"Positive Accuracy: {pos_acc.item():.4f}\")\n",
        "    print(f\"Negative Accuracy: {neg_acc.item():.4f}\")\n",
        "    print(f\"Overall Accuracy: {overall_acc.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from ogb.nodeproppred import NodePropPredDataset\n",
        "from torch_geometric.nn import GCNConv, SAGEConv\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import os\n",
        "\n",
        "gcn_ckpt_path = \"/content/drive/MyDrive/gcn_node_cls.pt\"\n",
        "sage_ckpt_path = \"/content/drive/MyDrive/sage_node_cls.pt\"\n",
        "edge_ckpt_path = \"/content/drive/MyDrive/sage_edge_pred.pt\"\n",
        "\n",
        "\n",
        "# 1. Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
        "dataset = NodePropPredDataset(name='ogbn-products')\n",
        "split_idx = dataset.get_idx_split()\n",
        "\n",
        "graph, labels = dataset[0]\n",
        "edge_index = torch.tensor(graph['edge_index'], dtype=torch.long)\n",
        "x = torch.tensor(graph['node_feat'], dtype=torch.float)\n",
        "y = torch.tensor(labels, dtype=torch.long).squeeze()\n",
        "\n",
        "# Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒØŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ùˆ ØªØ³Øª\n",
        "data = Data(x=x, edge_index=edge_index, y=y)\n",
        "data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
        "data.val_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
        "data.test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
        "\n",
        "data.train_mask[split_idx[\"train\"]] = True\n",
        "data.val_mask[split_idx[\"valid\"]] = True\n",
        "data.test_mask[split_idx[\"test\"]] = True\n",
        "\n",
        "# 2. ØªØ¹Ø±ÛŒÙ Ù…Ø¯Ù„â€ŒÙ‡Ø§\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "        self.dropout = torch.nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
        "        self.dropout = torch.nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# 3. ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ù…ÙˆØ²Ø´\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "data = data.to(device)\n",
        "\n",
        "# def train_model(model, data, epochs=5):\n",
        "#     model = model.to(device)\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "#     train_losses, val_accs, val_f1s = [], [], []\n",
        "\n",
        "#     for epoch in range(epochs):\n",
        "#         model.train()\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         out = model(data)\n",
        "#         loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ\n",
        "#         model.eval()\n",
        "#         with torch.no_grad():\n",
        "#             out = model(data)\n",
        "#             pred = out.argmax(dim=1)\n",
        "\n",
        "#             # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¯Ù‚Øª Ùˆ F1 Ø¨Ø±Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ\n",
        "#             val_acc = accuracy_score(data.y[data.val_mask].cpu(),\n",
        "#                                    pred[data.val_mask].cpu())\n",
        "#             val_f1 = f1_score(data.y[data.val_mask].cpu(),\n",
        "#                              pred[data.val_mask].cpu(), average='weighted')\n",
        "\n",
        "#         train_losses.append(loss.item())\n",
        "#         val_accs.append(val_acc)\n",
        "#         val_f1s.append(val_f1)\n",
        "\n",
        "#         if (epoch + 1) % 10 == 0:\n",
        "#             print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, '\n",
        "#                   f'Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}')\n",
        "\n",
        "#     return model, train_losses, val_accs, val_f1s\n",
        "\n",
        "def train_model(model, data, epochs=5, model_name=\"gcn\"):\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "    ckpt_path = gcn_ckpt_path if model_name == \"gcn\" else sage_ckpt_path\n",
        "    start_epoch = 0\n",
        "\n",
        "    if os.path.exists(ckpt_path):\n",
        "        checkpoint = torch.load(ckpt_path)\n",
        "        model.load_state_dict(checkpoint['model'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        print(f\"âœ… Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ {model_name} Ø§Ø² epoch {start_epoch}\")\n",
        "\n",
        "    train_losses, val_accs, val_f1s = [], [], []\n",
        "\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ...\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "            val_acc = accuracy_score(data.y[data.val_mask].cpu(),\n",
        "                                     pred[data.val_mask].cpu())\n",
        "            val_f1 = f1_score(data.y[data.val_mask].cpu(),\n",
        "                             pred[data.val_mask].cpu(), average='weighted')\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "        val_accs.append(val_acc)\n",
        "        val_f1s.append(val_f1)\n",
        "\n",
        "        print(f'[Epoch {epoch+1}] Loss: {loss.item():.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}')\n",
        "\n",
        "        # ğŸ’¾ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "        }, ckpt_path)\n",
        "\n",
        "    return model, train_losses, val_accs, val_f1s\n",
        "\n",
        "\n",
        "# 4. Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§\n",
        "print(\"\\nØ¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ GraphSAGE:\")\n",
        "# sage_model, sage_loss, sage_val_acc, sage_val_f1 = train_model(\n",
        "#     GraphSAGE(data.num_features, 256, dataset.num_classes),\n",
        "#     data\n",
        "# )\n",
        "\n",
        "sage_model, sage_loss, sage_val_acc, sage_val_f1 = train_model(\n",
        "    GraphSAGE(data.num_features, 256, dataset.num_classes), data, model_name=\"sage\"\n",
        ")\n",
        "\n",
        "print(\"\\nØ¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ GCN:\")\n",
        "# gcn_model, gcn_loss, gcn_val_acc, gcn_val_f1 = train_model(\n",
        "#     GCN(data.num_features, 256, dataset.num_classes),\n",
        "#     data\n",
        "# )\n",
        "\n",
        "gcn_model, gcn_loss, gcn_val_acc, gcn_val_f1 = train_model(\n",
        "    GCN(data.num_features, 256, dataset.num_classes), data, model_name=\"gcn\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# 5. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ\n",
        "def evaluate_model(model, data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data)\n",
        "        pred = out.argmax(dim=1)\n",
        "\n",
        "        train_acc = accuracy_score(data.y[data.train_mask].cpu(),\n",
        "                                 pred[data.train_mask].cpu())\n",
        "        val_acc = accuracy_score(data.y[data.val_mask].cpu(),\n",
        "                               pred[data.val_mask].cpu())\n",
        "        test_acc = accuracy_score(data.y[data.test_mask].cpu(),\n",
        "                                pred[data.test_mask].cpu())\n",
        "\n",
        "        test_f1 = f1_score(data.y[data.test_mask].cpu(),\n",
        "                         pred[data.test_mask].cpu(), average='weighted')\n",
        "\n",
        "        return train_acc, val_acc, test_acc, test_f1\n",
        "\n",
        "print(\"\\nØ§Ø±Ø²ÛŒØ§Ø¨ÛŒ GraphSAGE:\")\n",
        "sage_train_acc, sage_val_acc, sage_test_acc, sage_test_f1 = evaluate_model(sage_model, data)\n",
        "print(f\"Train Acc: {sage_train_acc:.4f}, Val Acc: {sage_val_acc:.4f}, \"\n",
        "      f\"Test Acc: {sage_test_acc:.4f}, Test F1: {sage_test_f1:.4f}\")\n",
        "\n",
        "print(\"\\nØ§Ø±Ø²ÛŒØ§Ø¨ÛŒ GCN:\")\n",
        "gcn_train_acc, gcn_val_acc, gcn_test_acc, gcn_test_f1 = evaluate_model(gcn_model, data)\n",
        "print(f\"Train Acc: {gcn_train_acc:.4f}, Val Acc: {gcn_val_acc:.4f}, \"\n",
        "      f\"Test Acc: {gcn_test_acc:.4f}, Test F1: {gcn_test_f1:.4f}\")\n",
        "\n",
        "# 6. Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Ù†Ù…ÙˆØ¯Ø§Ø± Ø®Ø·Ø§\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(sage_loss, label='GraphSAGE')\n",
        "plt.plot(gcn_loss, label='GCN')\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Ù†Ù…ÙˆØ¯Ø§Ø± Ø¯Ù‚Øª Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(sage_val_acc, label='GraphSAGE')\n",
        "plt.plot(gcn_val_acc, label='GCN')\n",
        "plt.title('Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Ù…Ù‚Ø§ÛŒØ³Ù‡ F1\n",
        "plt.subplot(2, 2, 3)\n",
        "models = ['GraphSAGE', 'GCN']\n",
        "test_f1 = [sage_test_f1, gcn_test_f1]\n",
        "plt.bar(models, test_f1, color=['blue', 'orange'])\n",
        "plt.title('Test F1-Score Comparison')\n",
        "plt.ylabel('F1-Score')\n",
        "\n",
        "# Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¯Ù‚Øª ØªØ³Øª\n",
        "plt.subplot(2, 2, 4)\n",
        "test_acc = [sage_test_acc, gcn_test_acc]\n",
        "plt.bar(models, test_acc, color=['blue', 'orange'])\n",
        "plt.title('Test Accuracy Comparison')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('results_comparison.png')\n",
        "plt.show()\n",
        "\n",
        "# 7. Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ø¶Ø§ÙÛŒ: Edge Prediction\n",
        "class EdgePredictor(torch.nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(EdgePredictor, self).__init__()\n",
        "        self.lin1 = torch.nn.Linear(2 * in_channels, 128)\n",
        "        self.lin2 = torch.nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, z, edge_index):\n",
        "        src, dst = edge_index\n",
        "        x = torch.cat([z[src], z[dst]], dim=1)\n",
        "        x = F.relu(self.lin1(x))\n",
        "        return torch.sigmoid(self.lin2(x)).squeeze()\n",
        "\n",
        "def get_embeddings(model, data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ embeddings Ø§Ø² Ù„Ø§ÛŒÙ‡ Ø§ÙˆÙ„\n",
        "        embeddings = model.conv1(data.x, data.edge_index)\n",
        "        embeddings = F.relu(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "print(\"\\nØ¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Edge Prediction Ø¨Ø§ GraphSAGE:\")\n",
        "sage_embeddings = get_embeddings(sage_model, data)\n",
        "edge_model = EdgePredictor(256).to(device)\n",
        "optimizer = torch.optim.Adam(edge_model.parameters(), lr=0.01)\n",
        "\n",
        "# Ù†Ù…ÙˆÙ†Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø² ÛŒØ§Ù„Ù‡Ø§ÛŒ Ù…Ù†ÙÛŒ\n",
        "def negative_sampling(edge_index, num_nodes, num_neg_samples=None):\n",
        "    if num_neg_samples is None:\n",
        "        num_neg_samples = edge_index.size(1)\n",
        "\n",
        "    neg_edge_index = torch.randint(0, num_nodes, (2, num_neg_samples), device=device)\n",
        "    return neg_edge_index\n",
        "\n",
        "# for epoch in range(5):\n",
        "#     edge_model.train()\n",
        "#     optimizer.zero_grad()\n",
        "\n",
        "#     # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø±Ø§ÛŒ ÛŒØ§Ù„Ù‡Ø§ÛŒ Ù…Ø«Ø¨Øª\n",
        "#     pos_pred = edge_model(sage_embeddings, data.edge_index)\n",
        "#     pos_loss = F.binary_cross_entropy(pos_pred, torch.ones_like(pos_pred))\n",
        "\n",
        "#     # Ù†Ù…ÙˆÙ†Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ùˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø±Ø§ÛŒ ÛŒØ§Ù„Ù‡Ø§ÛŒ Ù…Ù†ÙÛŒ\n",
        "#     neg_edge_index = negative_sampling(data.edge_index, data.num_nodes, num_neg_samples=data.edge_index.size(1))\n",
        "#     neg_pred = edge_model(sage_embeddings, neg_edge_index)\n",
        "#     neg_loss = F.binary_cross_entropy(neg_pred, torch.zeros_like(neg_pred))\n",
        "\n",
        "#     loss = pos_loss + neg_loss\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "\n",
        "#     if (epoch + 1) % 10 == 0:\n",
        "#         print(f'Epoch {epoch+1}/50, Loss: {loss.item():.4f}')\n",
        "\n",
        "edge_ckpt_path = \"/content/drive/MyDrive/sage_edge_pred.pt\"\n",
        "start_epoch = 0\n",
        "\n",
        "if os.path.exists(edge_ckpt_path):\n",
        "    checkpoint = torch.load(edge_ckpt_path)\n",
        "    edge_model.load_state_dict(checkpoint['model'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    print(f\"âœ… Edge predictor Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯ Ø§Ø² epoch {start_epoch}\")\n",
        "else:\n",
        "    print(\"ğŸŸ¡ Ø¢Ù…ÙˆØ²Ø´ edge predictor Ø§Ø² Ø§Ø¨ØªØ¯Ø§\")\n",
        "\n",
        "for epoch in range(start_epoch, 50):\n",
        "    edge_model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    pos_pred = edge_model(sage_embeddings, data.edge_index)\n",
        "    pos_loss = F.binary_cross_entropy(pos_pred, torch.ones_like(pos_pred))\n",
        "\n",
        "    neg_edge_index = negative_sampling(data.edge_index, data.num_nodes, num_neg_samples=data.edge_index.size(1))\n",
        "    neg_pred = edge_model(sage_embeddings, neg_edge_index)\n",
        "    neg_loss = F.binary_cross_entropy(neg_pred, torch.zeros_like(neg_pred))\n",
        "\n",
        "    loss = pos_loss + neg_loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/50, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model': edge_model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict()\n",
        "    }, edge_ckpt_path)\n",
        "\n",
        "\n",
        "# Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Edge Prediction\n",
        "edge_model.eval()\n",
        "with torch.no_grad():\n",
        "    pos_pred = edge_model(sage_embeddings, data.edge_index)\n",
        "    neg_edge_index = negative_sampling(data.edge_index, data.num_nodes, num_neg_samples=100000)\n",
        "    neg_pred = edge_model(sage_embeddings, neg_edge_index)\n",
        "\n",
        "    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¯Ù‚Øª\n",
        "    pos_acc = (pos_pred > 0.5).float().mean()\n",
        "    neg_acc = (neg_pred < 0.5).float().mean()\n",
        "    overall_acc = (pos_acc * pos_pred.size(0) + neg_acc * neg_pred.size(0)) / (pos_pred.size(0) + neg_pred.size(0))\n",
        "\n",
        "    print(f\"\\nÙ†ØªØ§ÛŒØ¬ Edge Prediction:\")\n",
        "    print(f\"Positive Accuracy: {pos_acc.item():.4f}\")\n",
        "    print(f\"Negative Accuracy: {neg_acc.item():.4f}\")\n",
        "    print(f\"Overall Accuracy: {overall_acc.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "yp7QCY_iubuq",
        "outputId": "a0cc47bd-c202-4410-dee7-18b7b94e337b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch_geometric'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2905537413>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mogb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodeproppred\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNodePropPredDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGCNConv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSAGEConv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_geometric'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}