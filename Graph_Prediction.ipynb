{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeQivnZVaf8_"
      },
      "outputs": [],
      "source": [
        "# Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡ ØªØ³Øª\n",
        "sage_test_acc, sage_test_f1 = evaluate(sage_model, data, test_idx)\n",
        "gcn_test_acc, gcn_test_f1 = evaluate(gcn_model, data, test_idx)\n",
        "\n",
        "print(f'GraphSAGE - Test Accuracy: {sage_test_acc:.4f}, Test F1: {sage_test_f1:.4f}')\n",
        "print(f'GCN - Test Accuracy: {gcn_test_acc:.4f}, Test F1: {gcn_test_f1:.4f}')\n",
        "\n",
        "# Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(sage_train_loss, label='GraphSAGE')\n",
        "plt.plot(gcn_train_loss, label='GCN')\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(sage_val_acc, label='GraphSAGE Val Acc')\n",
        "plt.plot(gcn_val_acc, label='GCN Val Acc')\n",
        "plt.title('Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuTpIVJJavYp"
      },
      "source": [
        "# Edge Pr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8hs4xDtatwO"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import LinkPrediction\n",
        "from torch_geometric.utils import negative_sampling\n",
        "\n",
        "class EdgePredictor(torch.nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.lin1 = torch.nn.Linear(2 * in_channels, 128)\n",
        "        self.lin2 = torch.nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, z, edge_index):\n",
        "        src, dst = edge_index\n",
        "        x = torch.cat([z[src], z[dst]], dim=1)\n",
        "        x = F.relu(self.lin1(x))\n",
        "        return torch.sigmoid(self.lin2(x)).view(-1)\n",
        "\n",
        "# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² embeddings ÛŒØ§Ø¯Ú¯Ø±ÙØªÙ‡ Ø´Ø¯Ù‡ ØªÙˆØ³Ø· GraphSAGE\n",
        "edge_model = EdgePredictor(hidden_channels)\n",
        "optimizer = torch.optim.Adam(edge_model.parameters(), lr=0.01)\n",
        "\n",
        "# Ø¢Ù…ÙˆØ²Ø´ edge prediction\n",
        "for epoch in range(50):\n",
        "    edge_model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # ØªÙˆÙ„ÛŒØ¯ embeddings Ø¨Ø§ GraphSAGE\n",
        "    z = sage_model.conv1(data.x, data.edge_index)\n",
        "    z = sage_model.conv2(z, data.edge_index)\n",
        "\n",
        "    # Ù†Ù…ÙˆÙ†Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø² ÛŒØ§Ù„Ù‡Ø§ÛŒ Ù…Ø«Ø¨Øª Ùˆ Ù…Ù†ÙÛŒ\n",
        "    pos_edge_index = data.edge_index\n",
        "    neg_edge_index = negative_sampling(data.edge_index, num_nodes=data.num_nodes)\n",
        "\n",
        "    # Ù…Ø­Ø§Ø³Ø¨Ù‡ loss\n",
        "    pos_pred = edge_model(z, pos_edge_index)\n",
        "    neg_pred = edge_model(z, neg_edge_index)\n",
        "\n",
        "    pos_loss = -torch.log(pos_pred + 1e-15).mean()\n",
        "    neg_loss = -torch.log(1 - neg_pred + 1e-15).mean()\n",
        "    loss = pos_loss + neg_loss\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hc8Zf6tcI86"
      },
      "source": [
        "-----------------------------\n",
        "# Temp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ù†ØµØ¨ numpy Ø³Ø§Ø²Ú¯Ø§Ø±\n",
        "!pip install numpy==1.24.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "7dRLBy1Z64nB",
        "outputId": "99d921f7-5547-4dd1-9ab0-6d910491e790",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.24.4\n",
            "  Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.4 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "ce159667d86d488b97dd128c68307c00"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ù†ØµØ¨ Ù†Ø³Ø®Ù‡ Ø³Ø§Ø²Ú¯Ø§Ø± PyTorch\n",
        "!pip install torch==2.0.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ø¨Ø³ØªÙ‡ Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ù…Ø·Ø§Ø¨Ù‚ PyG ØªÙˆØµÛŒÙ‡â€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ torch==2.0.1\n",
        "!pip install pyg-lib==0.2.0 torch-scatter==2.1.1 torch-sparse==0.6.17 torch-cluster==1.6.1 torch-spline-conv==1.2.2 -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
        "\n",
        "# Ù†ØµØ¨ Ù†Ø³Ø®Ù‡ ØµØ­ÛŒØ­ PyG\n",
        "!pip install torch-geometric==2.3.1\n",
        "\n",
        "# Ù†ØµØ¨ OGB\n",
        "!pip install ogb\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSuy5VjbgKNV",
        "outputId": "ebf0d709-8e8a-4020-9022-6488148c4696",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.0.1\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp311-cp311-linux_x86_64.whl (2267.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cpu)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (4.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.1.6)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cmake (from triton==2.0.0->torch==2.0.1)\n",
            "  Downloading https://download.pytorch.org/whl/cmake-3.25.0-py2.py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lit (from triton==2.0.0->torch==2.0.1)\n",
            "  Downloading https://download.pytorch.org/whl/lit-15.0.7.tar.gz (132 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.24.4)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.22.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.20.1%2Bcu118-cp311-cp311-linux_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m113.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.20.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.19.1%2Bcu118-cp311-cp311-linux_x86_64.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m114.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.19.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is still looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.18.1%2Bcu118-cp311-cp311-linux_x86_64.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m117.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.18.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.17.2%2Bcu118-cp311-cp311-linux_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.17.1%2Bcu118-cp311-cp311-linux_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.17.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.32.3)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.16.2%2Bcu118-cp311-cp311-linux_x86_64.whl (6.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m114.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.16.1%2Bcu118-cp311-cp311-linux_x86_64.whl (6.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.16.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.15.2%2Bcu118-cp311-cp311-linux_x86_64.whl (6.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m118.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (6.6 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.5.1%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.5.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.4.1%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.4.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is still looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.3.1%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.3.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.2.2%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.2.1%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.2.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.1.2%2Bcu118-cp311-cp311-linux_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.1.1%2Bcu118-cp311-cp311-linux_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.1.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.0.2%2Bcu118-cp311-cp311-linux_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "Building wheels for collected packages: lit\n",
            "  Building wheel for lit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-15.0.7-py3-none-any.whl size=89991 sha256=4db81ab8f2f1bec96e38ded77668e64fe784f94a9e6b8845f089f7ca388b3a59\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/5d/45/34fe9945d5e45e261134e72284395be36c2d4828af38e2b0fe\n",
            "Successfully built lit\n",
            "Installing collected packages: lit, cmake, triton, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cpu\n",
            "    Uninstalling torch-2.6.0+cpu:\n",
            "      Successfully uninstalled torch-2.6.0+cpu\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cpu\n",
            "    Uninstalling torchvision-0.21.0+cpu:\n",
            "      Successfully uninstalled torchvision-0.21.0+cpu\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.6.0+cpu\n",
            "    Uninstalling torchaudio-2.6.0+cpu:\n",
            "      Successfully uninstalled torchaudio-2.6.0+cpu\n",
            "Successfully installed cmake-3.25.0 lit-15.0.7 torch-2.0.1+cu118 torchaudio-2.0.2+cu118 torchvision-0.15.2+cu118 triton-2.0.0\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
            "Collecting pyg-lib==0.2.0\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/pyg_lib-0.2.0%2Bpt20cu118-cp311-cp311-linux_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-scatter==2.1.1\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_scatter-2.1.1%2Bpt20cu118-cp311-cp311-linux_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m122.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-sparse==0.6.17\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_sparse-0.6.17%2Bpt20cu118-cp311-cp311-linux_x86_64.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-cluster==1.6.1\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_cluster-1.6.1%2Bpt20cu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-spline-conv==1.2.2\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_spline_conv-1.2.2%2Bpt20cu118-cp311-cp311-linux_x86_64.whl (886 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m886.5/886.5 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse==0.6.17) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse==0.6.17) (1.24.4)\n",
            "Installing collected packages: torch-spline-conv, torch-scatter, pyg-lib, torch-sparse, torch-cluster\n",
            "Successfully installed pyg-lib-0.2.0+pt20cu118 torch-cluster-1.6.1+pt20cu118 torch-scatter-2.1.1+pt20cu118 torch-sparse-0.6.17+pt20cu118 torch-spline-conv-1.2.2+pt20cu118\n",
            "Collecting torch-geometric==2.3.1\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.3.1) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.3.1) (1.24.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.3.1) (1.15.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.3.1) (3.1.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.3.1) (2.32.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.3.1) (3.2.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.3.1) (1.6.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.3.1) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric==2.3.1) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.3.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.3.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.3.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.3.1) (2025.4.26)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->torch-geometric==2.3.1) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->torch-geometric==2.3.1) (3.6.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910496 sha256=a9d71189ecaa821c3cf10e17cb8e9aabd5283cf2a32885135c883a49cd184a24\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/78/6c/9fd091ca1c5e137c66cbd03696ffa14f75e9abc5abfe0dbcc6\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.3.1\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (1.24.4)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (1.6.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.4.0)\n",
            "Collecting outdated>=0.2.0 (from ogb)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.11/dist-packages (from outdated>=0.2.0->ogb) (75.2.0)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
            "  Downloading littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from outdated>=0.2.0->ogb) (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (4.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.1.6)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb) (3.25.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb) (15.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->ogb) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Downloading littleutils-0.2.4-py3-none-any.whl (8.1 kB)\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.4 ogb-1.3.6 outdated-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "from torch_geometric.utils import to_undirected\n",
        "\n",
        "# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯ÛŒØªØ§Ø³Øª ogbn-products\n",
        "dataset = PygNodePropPredDataset(name='ogbn-products')\n",
        "data = dataset[0]  # ÙÙ‚Ø· ÛŒÚ© Ø´ÛŒØ¡ Data Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯\n",
        "\n",
        "# ÛŒØ§Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø¯ÙˆÙ† Ø¬Ù‡Øª Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…\n",
        "data.edge_index = to_undirected(data.edge_index)\n",
        "\n",
        "# ØªØ¨Ø¯ÛŒÙ„ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ Ø¨Ù‡ [num_nodes]\n",
        "data.y = data.y.squeeze()\n",
        "\n",
        "# Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ØŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒØŒ Ø¢Ø²Ù…ÙˆÙ†\n",
        "split_idx = dataset.get_idx_split()\n",
        "train_idx = split_idx['train']\n",
        "val_idx = split_idx['valid']\n",
        "test_idx = split_idx['test']\n",
        "\n",
        "# Ø¨Ø±Ø±Ø³ÛŒ Ø§ÙˆÙ„ÛŒÙ‡\n",
        "print(data)\n",
        "print(f\"# Train samples: {train_idx.shape[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtZDu6LLxbsx",
        "outputId": "21d67ba9-f0d1-4574-9423-3a06e6ab7671"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This will download 1.38GB. Will you proceed? (y/N)\n",
            "y\n",
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/products.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 1.38 GB: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1414/1414 [00:29<00:00, 47.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/products.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.64s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 3323.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(num_nodes=2449029, edge_index=[2, 123718152], x=[2449029, 100], y=[2449029])\n",
            "# Train samples: 196615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ø³Ø±ÛŒØ¹ Ø³Ø§Ø²:"
      ],
      "metadata": {
        "id": "Dpnp7UMqVkQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import NeighborLoader\n",
        "from torch import tensor\n",
        "\n",
        "# data = dataset[0]\n",
        "# data.edge_index = to_undirected(data.edge_index)\n",
        "# data.y = data.y.squeeze()\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# # ÙÙ‚Ø· Ù‡Ù…ÛŒÙ† Ø¨Ø®Ø´ Ø±ÙˆÛŒ GPU ÛŒØ§ CPU Ù…Ù†ØªÙ‚Ù„ Ù…ÛŒØ´Ù‡\n",
        "# data = data.to(device)\n",
        "\n",
        "train_idx = split_idx['train'].clone().detach()\n",
        "val_idx = split_idx['valid'].clone().detach()\n",
        "test_idx = split_idx['test'].clone().detach()\n",
        "\n",
        "train_loader = NeighborLoader(\n",
        "    data,\n",
        "    input_nodes=train_idx,\n",
        "    num_neighbors=[5, 3],\n",
        "    batch_size=256,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_loader = NeighborLoader(\n",
        "    data,\n",
        "    input_nodes=val_idx,\n",
        "    num_neighbors=[5, 3],\n",
        "    batch_size=256\n",
        ")\n",
        "\n",
        "test_loader = NeighborLoader(\n",
        "    data,\n",
        "    input_nodes=test_idx,\n",
        "    num_neighbors=[5, 3],\n",
        "    batch_size=256\n",
        ")\n"
      ],
      "metadata": {
        "id": "o0GDZfFqRtEO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GCN"
      ],
      "metadata": {
        "id": "_BNOvm_tEXG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "# ØªØ¹Ø±ÛŒÙ Ù…Ø¯Ù„ GCN Ø¯Ùˆ Ù„Ø§ÛŒÙ‡\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, num_features, hidden_channels, num_classes):\n",
        "        super(GCN, self).__init__()\n",
        "        # Ù„Ø§ÛŒÙ‡ Ø§ÙˆÙ„: Ø§Ø² ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ù‡ ÙØ¶Ø§ÛŒ Ù¾Ù†Ù‡Ø§Ù†\n",
        "        self.conv1 = GCNConv(num_features, hidden_channels, bias=False)\n",
        "        # Ù„Ø§ÛŒÙ‡ Ø¯ÙˆÙ…: Ø§Ø² ÙØ¶Ø§ÛŒ Ù¾Ù†Ù‡Ø§Ù† Ø¨Ù‡ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§\n",
        "        self.conv2 = GCNConv(hidden_channels, num_classes, bias=True)\n",
        "\n",
        "    # def forward(self, data):\n",
        "    #     x, edge_index = data.x, data.edge_index\n",
        "    #     x = self.conv1(x, edge_index)\n",
        "    #     x = F.relu(x)\n",
        "    #     x = F.dropout(x, p=0.5, training=self.training)\n",
        "    #     x = self.conv2(x, edge_index)\n",
        "    #     return x\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Dropout Ú©Ù…ØªØ± Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª (Ùˆ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø§ÙØª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¯Ø± Ø¯ÛŒØªØ§ÛŒ Ø²ÛŒØ§Ø¯)\n",
        "        x = F.dropout(x, p=0.3, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Bwoo2by8D7Fo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMQwc7qrFQ2z",
        "outputId": "95dfc9e3-afd9-423d-d8bd-b7a4d276dc35"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ù…Ù‚Ø§Ø¯ÛŒØ± Ø§Ø² Ø¯ÛŒØªØ§Ø³Øª\n",
        "num_features = data.num_node_features       # =100\n",
        "num_classes = int(data.y.max().item()) + 1  # =47\n",
        "hidden_channels = 32                       # Ù‚Ø§Ø¨Ù„ ØªÙ†Ø¸ÛŒÙ…\n",
        "\n",
        "#-----------------------------------------------------\n",
        "save_path = \"/content/drive/MyDrive/gcn_node_last.pt\"\n",
        "#-----------------------------------------------------\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Ø³Ø§Ø®Øª Ù…Ø¯Ù„\n",
        "model = GCN(num_features, hidden_channels, num_classes).to(device)\n",
        "data = data.to(device)\n",
        "train_idx = train_idx.to(device)\n",
        "val_idx = val_idx.to(device)\n",
        "test_idx = test_idx.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "#----------------------------------------------------- Ú¯ÙˆÚ¯Ù„ Ú©ÙˆÙ„Ø¨\n",
        "start_epoch = 1\n",
        "try:\n",
        "    # Ø§Ú¯Ø± ÙØ§ÛŒÙ„ÛŒ Ø§Ø² Ù‚Ø¨Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯ØŒ Ø§Ø² Ø§Ø¯Ø§Ù…Ù‡ Ø§Ø¬Ø±Ø§ Ú©Ù†\n",
        "    checkpoint = torch.load(\"gcn_node_last.pt\")\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    print(f\"âœ… Ø§Ø¯Ø§Ù…Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø§Ø² epoch {start_epoch}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"â³ Ø¢Ù…ÙˆØ²Ø´ Ø§Ø² Ø§ÙˆÙ„ Ø´Ø±ÙˆØ¹ Ù…ÛŒâ€ŒØ´ÙˆØ¯\")\n",
        "\n",
        "#----------------------------------------------------- Ú¯ÙˆÚ¯Ù„ Ø¯Ø±Ø§ÛŒÙˆ\n",
        "\n",
        "start_epoch = 1\n",
        "import os\n",
        "if os.path.exists(save_path):\n",
        "    checkpoint = torch.load(save_path)\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    print(f\"âœ… Ø§Ø¯Ø§Ù…Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø§Ø² epoch {start_epoch}\")\n",
        "else:\n",
        "    print(\"ğŸŸ¡ ÙØ§ÛŒÙ„ Ù…Ø¯Ù„ Ù‚Ø¨Ù„ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø¢Ù…ÙˆØ²Ø´ Ø§Ø² Ø§Ø¨ØªØ¯Ø§ Ø¢ØºØ§Ø² Ù…ÛŒâ€ŒØ´ÙˆØ¯.\")\n",
        "#-----------------------------------------------------\n",
        "\n",
        "# def train():\n",
        "#     model.train()\n",
        "#     optimizer.zero_grad()\n",
        "#     out = model(data)\n",
        "#     loss = F.cross_entropy(out[train_idx], data.y[train_idx])\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     return loss.item()\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch.x, batch.edge_index)\n",
        "        loss = F.cross_entropy(out, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "# @torch.no_grad()\n",
        "# def test():\n",
        "#     model.eval()\n",
        "#     out = model(data)\n",
        "#     pred = out.argmax(dim=1)\n",
        "\n",
        "#     accs = []\n",
        "#     for idx in [train_idx, val_idx, test_idx]:\n",
        "#         correct = (pred[idx] == data.y[idx]).sum().item()\n",
        "#         acc = correct / idx.shape[0]\n",
        "#         accs.append(acc)\n",
        "#     return accs\n",
        "@torch.no_grad()\n",
        "def test(loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        out = model(batch.x, batch.edge_index)\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += (pred == batch.y).sum().item()\n",
        "        total += batch.y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "\n",
        "final_epoch = 3  # ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø§Ù¾ÙˆÚ©â€ŒÙ‡Ø§\n",
        "\n",
        "for epoch in range(start_epoch, final_epoch + 1):\n",
        "    loss = train()\n",
        "    train_acc = test(train_loader)\n",
        "    val_acc = test(val_loader)\n",
        "    test_acc = test(test_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch:03d}, Loss: {loss:.4f}, \"\n",
        "          f\"Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), f\"gcn_node_epoch_{epoch:03d}.pt\")\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict()\n",
        "    }, save_path)\n",
        "    print(f\"ğŸ’¾ Ù…Ø¯Ù„ Ø¯Ø± Google Drive Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: epoch {epoch}\")\n",
        "\n",
        "    # Ù¾Ø§ÛŒØ§Ù† Ø¢Ù…ÙˆØ²Ø´\n",
        "    if epoch == final_epoch:\n",
        "        print(\" \")\n",
        "\n",
        "print(\"âœ… Ø¢Ù…ÙˆØ²Ø´ Ú©Ø§Ù…Ù„ Ø´Ø¯ Ùˆ Ù…Ø¯Ù„ Ù†Ù‡Ø§ÛŒÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª.\")"
      ],
      "metadata": {
        "id": "NaaE9NsJ4j5-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e868708-2f9f-4f06-abac-00424e38bdfc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â³ Ø¢Ù…ÙˆØ²Ø´ Ø§Ø² Ø§ÙˆÙ„ Ø´Ø±ÙˆØ¹ Ù…ÛŒâ€ŒØ´ÙˆØ¯\n",
            "âœ… Ø§Ø¯Ø§Ù…Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø§Ø² epoch 4\n",
            "âœ… Ø¢Ù…ÙˆØ²Ø´ Ú©Ø§Ù…Ù„ Ø´Ø¯ Ùˆ Ù…Ø¯Ù„ Ù†Ù‡Ø§ÛŒÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U scikit-learn --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VlKF3ar1jlz5",
        "outputId": "27848d47-9178-4bd1-f06a-35918fba6eb9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m119.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_gcn_all_metrics(model, loader, name=\"GCN\"):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        out = model(batch.x, batch.edge_index)\n",
        "        pred = out.argmax(dim=1)\n",
        "        all_preds.append(pred.cpu())\n",
        "        all_labels.append(batch.y.cpu())\n",
        "\n",
        "    y_true = torch.cat(all_labels).numpy()\n",
        "    y_pred = torch.cat(all_preds).numpy()\n",
        "\n",
        "    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
        "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "    print(f\"\\nğŸ“Š Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ {name}:\")\n",
        "    print(f\"Accuracy    : {acc:.4f}\")\n",
        "    print(f\"F1-Score(Macro): {f1_macro:.4f}\")\n",
        "    print(f\"F1-Score(Micro): {f1_micro:.4f}\")\n",
        "    print(f\"Precision   : {precision:.4f}\")\n",
        "    print(f\"Recall      : {recall:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"Accuracy\": acc,\n",
        "        \"F1-Macro\": f1_macro,\n",
        "        \"F1-Micro\": f1_micro,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall\n",
        "    }\n",
        "\n",
        "# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ GCN Ùˆ Ù„ÙˆØ¯Ø± ØªØ³Øª\n",
        "gcn_metrics = evaluate_gcn_all_metrics(model, test_loader, name=\"GCN\")\n",
        "evaluate_gcn_all_metrics(model, val_loader, name=\"GCN-Validation\")\n",
        "evaluate_gcn_all_metrics(model, train_loader, name=\"GCN-Train\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbTpQ6i0jIdw",
        "outputId": "32b94434-1e0d-4fd7-ff01-334f15e7134a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Š Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ GCN:\n",
            "Accuracy    : 0.5621\n",
            "F1-Score(Macro): 0.1960\n",
            "F1-Score(Micro): 0.5621\n",
            "Precision   : 0.2872\n",
            "Recall      : 0.1905\n",
            "\n",
            "ğŸ“Š Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ GCN-Validation:\n",
            "Accuracy    : 0.6560\n",
            "F1-Score(Macro): 0.2094\n",
            "F1-Score(Micro): 0.6560\n",
            "Precision   : 0.3084\n",
            "Recall      : 0.1943\n",
            "\n",
            "ğŸ“Š Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ GCN-Train:\n",
            "Accuracy    : 0.6537\n",
            "F1-Score(Macro): 0.2111\n",
            "F1-Score(Micro): 0.6537\n",
            "Precision   : 0.3139\n",
            "Recall      : 0.1961\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Accuracy': 0.6537282071309946,\n",
              " 'F1-Macro': 0.21113551812103565,\n",
              " 'F1-Micro': 0.6537282071309946,\n",
              " 'Precision': 0.31394407493096366,\n",
              " 'Recall': 0.1960981341524284}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, BatchNorm1d\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class OptimizedGCN(torch.nn.Module):\n",
        "    def __init__(self, num_features, hidden_channels, num_classes):\n",
        "        super().__init__()\n",
        "        self.input_proj = Linear(num_features, 64)\n",
        "        self.conv1 = GCNConv(64, hidden_channels)\n",
        "        self.bn1 = BatchNorm1d(hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.input_proj(x)\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.4, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "fTjuUc0OUPNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "#  Ù…Ø¯Ù„ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡ GCN\n",
        "class ImprovedGCN(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels, bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels, bias=True)\n",
        "        self.dropout = dropout\n",
        "        self.act = nn.LeakyReLU(negative_slope=0.1)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.conv1.reset_parameters()\n",
        "        self.conv2.reset_parameters()\n",
        "        self.bn1.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = self.act(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "#  Label Smoothing Loss\n",
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    def __init__(self, eps=0.1):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        n_classes = pred.size(1)\n",
        "        log_preds = F.log_softmax(pred, dim=1)\n",
        "        target_onehot = F.one_hot(target, num_classes=n_classes).float()\n",
        "        target_smooth = target_onehot * (1 - self.eps) + self.eps / n_classes\n",
        "        loss = -(target_smooth * log_preds).sum(dim=1).mean()\n",
        "        return loss\n",
        "\n",
        "#  ØªÙ†Ø¸ÛŒÙ…Ø§Øª\n",
        "model = ImprovedGCN(num_features, hidden_channels, num_classes).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = LabelSmoothingCrossEntropy(eps=0.1)\n",
        "\n",
        "#  Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø§ Early Stopping (ØªØ§ 3 epoch Ø¨ÛŒâ€ŒØ¨Ù‡Ø¨ÙˆØ¯ â†’ ØªÙˆÙ‚Ù)\n",
        "best_val_acc = 0\n",
        "patience = 3\n",
        "wait = 0\n",
        "start_epoch = 1\n",
        "\n",
        "for epoch in range(start_epoch, start_epoch + 10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch.x, batch.edge_index)\n",
        "        loss = criterion(out, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    model.eval()\n",
        "    def eval(loader):\n",
        "        correct, total = 0, 0\n",
        "        for batch in loader:\n",
        "            batch = batch.to(device)\n",
        "            out = model(batch.x, batch.edge_index)\n",
        "            pred = out.argmax(dim=1)\n",
        "            correct += (pred == batch.y).sum().item()\n",
        "            total += batch.y.size(0)\n",
        "        return correct / total\n",
        "\n",
        "    train_acc = eval(train_loader)\n",
        "    val_acc = eval(val_loader)\n",
        "    test_acc = eval(test_loader)\n",
        "\n",
        "    print(f\"[Epoch {epoch:02d}] Loss: {total_loss:.4f} | Train: {train_acc:.4f} | Val: {val_acc:.4f} | Test: {test_acc:.4f}\")\n",
        "    torch.save(model.state_dict(), f\"Impgcn_node_epoch_{epoch:03d}.pt\")\n",
        "\n",
        "    #  Early Stopping Logic\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        wait = 0\n",
        "        torch.save(model.state_dict(), \"/content/drive/MyDrive/improved_gcn_best.pt\")\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= patience:\n",
        "            print(\" Ø¢Ù…ÙˆØ²Ø´ Ù…ØªÙˆÙ‚Ù Ø´Ø¯ (early stop).\")\n",
        "            break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "kDyelipG4Oi6",
        "outputId": "2520c860-77a5-488e-c4ee-b2bdf86b152d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-4110775579>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Epoch {epoch:02d}] Loss: {total_loss:.4f} | Train: {train_acc:.4f} | Val: {val_acc:.4f} | Test: {test_acc:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-4110775579>\u001b[0m in \u001b[0;36meval\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/loader/base.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/loader/node_loader.py\u001b[0m in \u001b[0;36mcollate_fn\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0minput_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNodeSamplerInput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_sampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_from_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_per_worker\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Execute `filter_fn` in the worker process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/sampler/neighbor_sampler.py\u001b[0m in \u001b[0;36msample_from_nodes\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNodeSamplerInput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     ) -> Union[SamplerOutput, HeteroSamplerOutput]:\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnode_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;31m# Edge-based sampling #####################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/sampler/neighbor_sampler.py\u001b[0m in \u001b[0;36mnode_sample\u001b[0;34m(inputs, sample_fn)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mseed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/sampler/neighbor_sampler.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, seed, seed_time, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m                 \u001b[0;31m# TODO (matthias) `return_edge_id` if edge features present\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                 \u001b[0;31m# TODO (matthias) Ideally, `seed` inherits dtype from `colptr`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m                 out = torch.ops.pyg.neighbor_sample(\n\u001b[0m\u001b[1;32m    283\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolptr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;31m# We save the function ptr as the `op` attribute on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# OpOverloadPacket to access it here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0;31m# TODO: use this to make a __dir__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Crach & no run"
      ],
      "metadata": {
        "id": "qtIkHvvT4kzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ù…Ù‚Ø§Ø¯ÛŒØ± Ø§Ø² Ø¯ÛŒØªØ§Ø³Øª\n",
        "num_features = data.num_node_features       # =100\n",
        "num_classes = int(data.y.max().item()) + 1  # =47\n",
        "hidden_channels = 128                       # Ù‚Ø§Ø¨Ù„ ØªÙ†Ø¸ÛŒÙ…\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Ø³Ø§Ø®Øª Ù…Ø¯Ù„\n",
        "model = GCN(num_features, hidden_channels, num_classes).to(device)\n",
        "data = data.to(device)\n",
        "train_idx = train_idx.to(device)\n",
        "val_idx = val_idx.to(device)\n",
        "test_idx = test_idx.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "# def train():\n",
        "#     model.train()\n",
        "#     optimizer.zero_grad()\n",
        "#     out = model(data)\n",
        "#     loss = F.cross_entropy(out[train_idx], data.y[train_idx])\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     return loss.item()\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch.x, batch.edge_index)\n",
        "        loss = F.cross_entropy(out, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "# @torch.no_grad()\n",
        "# def test():\n",
        "#     model.eval()\n",
        "#     out = model(data)\n",
        "#     pred = out.argmax(dim=1)\n",
        "\n",
        "#     accs = []\n",
        "#     for idx in [train_idx, val_idx, test_idx]:\n",
        "#         correct = (pred[idx] == data.y[idx]).sum().item()\n",
        "#         acc = correct / idx.shape[0]\n",
        "#         accs.append(acc)\n",
        "#     return accs\n",
        "@torch.no_grad()\n",
        "def test(loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        out = model(batch.x, batch.edge_index)\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += (pred == batch.y).sum().item()\n",
        "        total += batch.y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "for epoch in range(1, 31):  # ÙÙ‚Ø· 30 Ø¯ÙˆØ±Ù‡ Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª Ø§ÙˆÙ„ÛŒÙ‡\n",
        "    loss = train()\n",
        "    train_acc = test(train_loader)\n",
        "    val_acc = test(val_loader)\n",
        "    test_acc = test(test_loader)\n",
        "    print(f\"Epoch {epoch:03d}, Loss: {loss:.4f}, \"\n",
        "          f\"Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaeHiK4nEe5i",
        "outputId": "cf10a058-c3e9-4ca5-e171-7a083a2e0590"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 001, Loss: 1.4965, Train: 0.6650, Val: 0.6638, Test: 0.5794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GraphSAGE"
      ],
      "metadata": {
        "id": "zFBE7p6GNDoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YiaEf5ayIM2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import SAGEConv\n",
        "\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
        "\n",
        "    # def forward(self, data):\n",
        "    #     x, edge_index = data.x, data.edge_index\n",
        "    #     x = self.conv1(x, edge_index)\n",
        "    #     x = F.relu(x)\n",
        "    #     x = F.dropout(x, p=0.5, training=self.training)\n",
        "    #     x = self.conv2(x, edge_index)\n",
        "    #     return x\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.3, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "PsCUf037NBsA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sage_ckpt_path = \"/content/drive/MyDrive/sage_node_last.pt\"\n",
        "\n",
        "sage_model = GraphSAGE(num_features, hidden_channels, num_classes).to(device)\n",
        "optimizer = torch.optim.Adam(sage_model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "\n",
        "start_epoch = 1\n",
        "import os\n",
        "if os.path.exists(sage_ckpt_path):\n",
        "    checkpoint = torch.load(sage_ckpt_path)\n",
        "    sage_model.load_state_dict(checkpoint['model'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    print(f\"âœ… Ø§Ø¯Ø§Ù…Ù‡ Ø¢Ù…ÙˆØ²Ø´ GraphSAGE Ø§Ø² epoch {start_epoch}\")\n",
        "else:\n",
        "    print(\"ğŸŸ¡ Ø¢Ù…ÙˆØ²Ø´ GraphSAGE Ø§Ø² Ø§Ø¨ØªØ¯Ø§ Ø´Ø±ÙˆØ¹ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\")\n",
        "\n",
        "\n",
        "def train_sage():\n",
        "    sage_model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = sage_model(batch.x, batch.edge_index)\n",
        "        loss = F.cross_entropy(out, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_sage(loader):\n",
        "    sage_model.eval()\n",
        "    correct = total = 0\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        out = sage_model(batch.x, batch.edge_index)\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += (pred == batch.y).sum().item()\n",
        "        total += batch.y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "\n",
        "sage_train_acc_list = []\n",
        "sage_val_acc_list = []\n",
        "sage_test_acc_list = []\n",
        "\n",
        "\n",
        "final_epoch = 3  # Ú©Ù„ ØªØ¹Ø¯Ø§Ø¯ Ø§Ù¾ÙˆÚ©â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø±\n",
        "\n",
        "for epoch in range(start_epoch, final_epoch + 1):\n",
        "    loss = train_sage()\n",
        "    print(\"--------\")\n",
        "    train_acc = test_sage(train_loader)\n",
        "    val_acc = test_sage(val_loader)\n",
        "    test_acc = test_sage(test_loader)\n",
        "\n",
        "    print(f\"[GraphSAGE] Epoch {epoch:03d}, Loss: {loss:.4f}, \"\n",
        "          f\"Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}\")\n",
        "\n",
        "    # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ø¯Ø± Google Drive (Ø§Ø¯Ø§Ù…Ù‡â€ŒÙ¾Ø°ÛŒØ±)\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model': sage_model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict()\n",
        "    }, sage_ckpt_path)\n",
        "\n",
        "    # Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¯Ø± Ù…Ø³ÛŒØ± Ù…Ø­Ù„ÛŒ Ø¨Ø§ Ø´Ù…Ø§Ø±Ù‡ Ø§Ù¾ÙˆÚ©\n",
        "    torch.save(sage_model.state_dict(), f\"sage_node_epoch_{epoch:03d}.pt\")\n",
        "    print(f\"ğŸ’¾ Ù…Ø¯Ù„ GraphSAGE Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: epoch {epoch}\")\n",
        "\n",
        "    # Ø§Ú¯Ø± Ø¨Ù‡ Ø¢Ø®Ø± Ø§Ù¾ÙˆÚ© Ø±Ø³ÛŒØ¯:\n",
        "    if epoch == final_epoch:\n",
        "        print(\"âœ… Ø¢Ù…ÙˆØ²Ø´ Ú©Ø§Ù…Ù„ GraphSAGE Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯.\")\n",
        "\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "WywYu6c7NIrw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "659f059c-0fc4-4f74-81bb-f782d7692528"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Ø§Ø¯Ø§Ù…Ù‡ Ø¢Ù…ÙˆØ²Ø´ GraphSAGE Ø§Ø² epoch 4\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_sage_all_metrics(model, loader, name=\"GraphSAGE\"):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        out = model(batch.x, batch.edge_index)\n",
        "        pred = out.argmax(dim=1)\n",
        "        all_preds.append(pred.cpu())\n",
        "        all_labels.append(batch.y.cpu())\n",
        "\n",
        "    y_true = torch.cat(all_labels).numpy()\n",
        "    y_pred = torch.cat(all_preds).numpy()\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
        "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "    print(f\"\\nğŸ“Š Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ {name}:\")\n",
        "    print(f\"Accuracy      : {acc:.4f}\")\n",
        "    print(f\"F1-Score(Macro): {f1_macro:.4f}\")\n",
        "    print(f\"F1-Score(Micro): {f1_micro:.4f}\")\n",
        "    print(f\"Precision      : {precision:.4f}\")\n",
        "    print(f\"Recall         : {recall:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"Accuracy\": acc,\n",
        "        \"F1-Macro\": f1_macro,\n",
        "        \"F1-Micro\": f1_micro,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall\n",
        "    }\n",
        "\n",
        "# Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø±ÙˆÛŒ Ù…Ø¯Ù„ GraphSAGE Ùˆ Ù„ÙˆØ¯Ø± ØªØ³Øª\n",
        "sage_metrics = evaluate_sage_all_metrics(sage_model, test_loader, name=\"GraphSAGE\")\n",
        "evaluate_gcn_all_metrics(model, val_loader, name=\"SAGE-Validation\")\n",
        "evaluate_gcn_all_metrics(model, train_loader, name=\"SAGE-Train\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5oTM86NmGK_",
        "outputId": "3f4b9b66-1fee-4325-e96e-901f0b5e2e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Š Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ GraphSAGE:\n",
            "Accuracy      : 0.5690\n",
            "F1-Score(Macro): 0.2060\n",
            "F1-Score(Micro): 0.5690\n",
            "Precision      : 0.3068\n",
            "Recall         : 0.2011\n",
            "\n",
            "ğŸ“Š Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ SAGE-Validation:\n",
            "Accuracy    : 0.6023\n",
            "F1-Score(Macro): 0.1812\n",
            "F1-Score(Micro): 0.6023\n",
            "Precision   : 0.2893\n",
            "Recall      : 0.1614\n",
            "\n",
            "ğŸ“Š Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ SAGE-Train:\n",
            "Accuracy    : 0.6019\n",
            "F1-Score(Macro): 0.1806\n",
            "F1-Score(Micro): 0.6019\n",
            "Precision   : 0.2858\n",
            "Recall      : 0.1611\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Accuracy': 0.6019413583103213,\n",
              " 'F1-Macro': 0.1806062954188998,\n",
              " 'F1-Micro': 0.6019413583103213,\n",
              " 'Precision': 0.28575297160296087,\n",
              " 'Recall': 0.1611162531910095}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eval"
      ],
      "metadata": {
        "id": "YBt5XEnkN6ge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "@torch.no_grad()\n",
        "def collect_predictions(model, loader):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        out = model(batch.x, batch.edge_index)\n",
        "        pred = out.argmax(dim=1)\n",
        "        y_true.append(batch.y.cpu())\n",
        "        y_pred.append(pred.cpu())\n",
        "    return torch.cat(y_true), torch.cat(y_pred)\n",
        "\n",
        "# Ú¯Ø±ÙØªÙ† Ø®Ø±ÙˆØ¬ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§\n",
        "y_true_gcn, y_pred_gcn = collect_predictions(model, test_loader)\n",
        "y_true_sage, y_pred_sage = collect_predictions(sage_model, test_loader)\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"F1-Macro\": f1_score(y_true, y_pred, average='macro'),\n",
        "        \"F1-Micro\": f1_score(y_true, y_pred, average='micro'),\n",
        "        \"Precision\": precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
        "        \"Recall\": recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    }\n",
        "\n",
        "# Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§\n",
        "gcn_metrics = compute_metrics(y_true_gcn, y_pred_gcn)\n",
        "sage_metrics = compute_metrics(y_true_sage, y_pred_sage)\n",
        "\n",
        "# Ø³Ø§Ø®Øª Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ\n",
        "df = pd.DataFrame([gcn_metrics, sage_metrics], index=[\"GCN\", \"GraphSAGE\"])\n",
        "\n",
        "# Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ\n",
        "df.plot(kind='bar', figsize=(10, 6), colormap='tab10')\n",
        "plt.title(\"ğŸ“Š Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ GCN Ùˆ GraphSAGE\")\n",
        "plt.ylabel(\"Ù…Ù‚Ø¯Ø§Ø±\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Ù†Ù…Ø§ÛŒØ´ Ø¬Ø¯ÙˆÙ„ Ù†Ù‡Ø§ÛŒÛŒ\n",
        "print(\" Ø¬Ø¯ÙˆÙ„ Ù†Ù‡Ø§ÛŒÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡:\")\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "mKz9TMNHnBFd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "outputId": "3f47ba7e-d86f-41f7-dca6-9e1f47429c6a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-3224154149>:45: UserWarning: Glyph 128202 (\\N{BAR CHART}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 128202 (\\N{BAR CHART}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Ø¬Ø¯ÙˆÙ„ Ù†Ù‡Ø§ÛŒÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡:\n",
            "           Accuracy  F1-Macro  F1-Micro  Precision    Recall\n",
            "GCN        0.562116  0.196033  0.562116   0.287407  0.190544\n",
            "GraphSAGE  0.568981  0.205922  0.568981   0.306326  0.200950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra Point"
      ],
      "metadata": {
        "id": "-wjpHZLW5H1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Y-gMnLWtP1VT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  GCN"
      ],
      "metadata": {
        "id": "f_2jxfprSFNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------- Imports\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.utils import to_undirected, train_test_split_edges, negative_sampling\n",
        "from torch_geometric.data import Data\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# --------------------------- Dataset\n",
        "dataset = PygNodePropPredDataset(name='ogbn-products', root='/tmp/ogb')\n",
        "split_idx = dataset.get_idx_split()\n",
        "data = dataset[0]\n",
        "data.y = data.y.squeeze()\n",
        "data.edge_index = to_undirected(data.edge_index)\n",
        "data = train_test_split_edges(data, val_ratio=0.05, test_ratio=0.2)\n",
        "\n",
        "# --------------------------- Encoder\n",
        "class GCNEncoder(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, 64)\n",
        "        self.conv2 = GCNConv(64, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# --------------------------- Decoder\n",
        "def decode(z, edge_index):\n",
        "    return (z[edge_index[0]] * z[edge_index[1]]).sum(dim=1)\n",
        "\n",
        "# --------------------------- Loss (optimized)\n",
        "def compute_loss(z, pos_edge_index, num_nodes, batch_size=20000):\n",
        "    pos_score = decode(z, pos_edge_index)\n",
        "    pos_loss = -F.logsigmoid(pos_score).mean()\n",
        "\n",
        "    neg_batches = (pos_edge_index.size(1) // batch_size) + 1\n",
        "    neg_loss_total = 0.0\n",
        "\n",
        "    for _ in range(neg_batches):\n",
        "        neg_edge_index = negative_sampling(\n",
        "            edge_index=pos_edge_index,\n",
        "            num_nodes=num_nodes,\n",
        "            num_neg_samples=min(batch_size, pos_edge_index.size(1)),\n",
        "        )\n",
        "        neg_score = decode(z, neg_edge_index)\n",
        "        neg_loss_total += -F.logsigmoid(-neg_score).mean()\n",
        "\n",
        "    return pos_loss + neg_loss_total / neg_batches\n",
        "\n",
        "# --------------------------- Setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GCNEncoder(data.num_node_features, 64).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "x = data.x.to(device)\n",
        "train_edge_index = data.train_pos_edge_index.to(device)\n",
        "\n",
        "# --------------------------- Training\n",
        "save_path = \"/content/drive/MyDrive/edge_gcn_epoch_{epoch:03d}.pt\"\n",
        "for epoch in range(1, 4):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    z = model(x, train_edge_index)\n",
        "    loss = compute_loss(z, train_edge_index, x.size(0))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"[EdgePred] Epoch {epoch:02d} | Loss: {loss:.4f}\")\n",
        "    torch.save(model.state_dict(), save_path.format(epoch=epoch))\n",
        "\n",
        "# --------------------------- Evaluation\n",
        "@torch.no_grad()\n",
        "def evaluate(model, x, edge_index, pos_edge_index, neg_edge_index):\n",
        "    model.eval()\n",
        "    z = model(x, edge_index)\n",
        "    pos_score = torch.sigmoid(decode(z, pos_edge_index)).cpu().numpy()\n",
        "    neg_score = torch.sigmoid(decode(z, neg_edge_index)).cpu().numpy()\n",
        "    y_true = np.hstack([np.ones(pos_score.shape[0]), np.zeros(neg_score.shape[0])])\n",
        "    y_scores = np.hstack([pos_score, neg_score])\n",
        "    auc = roc_auc_score(y_true, y_scores)\n",
        "    ap = average_precision_score(y_true, y_scores)\n",
        "    return auc, ap\n",
        "\n",
        "# Prepare eval edges\n",
        "full_edge_index = data.edge_index.to(device)\n",
        "val_pos = data.val_pos_edge_index.to(device)\n",
        "val_neg = negative_sampling(full_edge_index, x.size(0), num_neg_samples=val_pos.size(1)).to(device)\n",
        "test_pos = data.test_pos_edge_index.to(device)\n",
        "test_neg = negative_sampling(full_edge_index, x.size(0), num_neg_samples=test_pos.size(1)).to(device)\n",
        "\n",
        "# Run evaluation\n",
        "val_auc, val_ap = evaluate(model, x, full_edge_index, val_pos, val_neg)\n",
        "test_auc, test_ap = evaluate(model, x, full_edge_index, test_pos, test_neg)\n",
        "print(f\"\\nğŸ“Š Validation AUC: {val_auc:.4f}, AP: {val_ap:.4f}\")\n",
        "print(f\"ğŸ“Š Test AUC     : {test_auc:.4f}, AP: {test_ap:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHiYmBKj5G46",
        "outputId": "852b79dc-4cbf-49ad-8280-5cd0c8d79c32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ogbn-products has been updated.\n",
            "Will you update the dataset now? (y/N)\n",
            "y\n",
            "This will download 1.38GB. Will you proceed? (y/N)\n",
            "y\n",
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/products.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 1.38 GB: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1414/1414 [00:26<00:00, 53.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /tmp/ogb/products.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Evaluation\n",
        "@torch.no_grad()\n",
        "def evaluate_edge_prediction(model, x, edge_index, pos_edge_index, neg_edge_index):\n",
        "    model.eval()\n",
        "    z = model(x, edge_index)\n",
        "    pos_scores = torch.sigmoid(decode(z, pos_edge_index)).cpu().numpy()\n",
        "    neg_scores = torch.sigmoid(decode(z, neg_edge_index)).cpu().numpy()\n",
        "    y_true = np.hstack([np.ones(pos_scores.shape[0]), np.zeros(neg_scores.shape[0])])\n",
        "    y_scores = np.hstack([pos_scores, neg_scores])\n",
        "    auc = roc_auc_score(y_true, y_scores)\n",
        "    ap = average_precision_score(y_true, y_scores)\n",
        "    return auc, ap\n",
        "\n",
        "full_edge_index = data.edge_index.to(device)\n",
        "val_pos = data.val_pos_edge_index.to(device)\n",
        "val_neg = negative_sampling(full_edge_index, x.size(0), num_neg_samples=val_pos.size(1))\n",
        "\n",
        "test_pos = data.test_pos_edge_index.to(device)\n",
        "test_neg = negative_sampling(full_edge_index, x.size(0), num_neg_samples=test_pos.size(1))\n",
        "\n",
        "val_auc, val_ap = evaluate_edge_prediction(model, x, full_edge_index, val_pos, val_neg)\n",
        "test_auc, test_ap = evaluate_edge_prediction(model, x, full_edge_index, test_pos, test_neg)\n",
        "\n",
        "print(f\"Validation AUC: {val_auc:.4f}, AP: {val_ap:.4f}\")\n",
        "print(f\"Test AUC: {test_auc:.4f}, AP: {test_ap:.4f}\")"
      ],
      "metadata": {
        "id": "0QgjFBAs6OcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SAGE"
      ],
      "metadata": {
        "id": "GaoedU5eSJkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.utils import to_undirected, train_test_split_edges, negative_sampling\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# 1. Load dataset\n",
        "dataset = PygNodePropPredDataset(name='ogbn-products', root='/tmp/ogb')\n",
        "graph = dataset[0]\n",
        "edge_index = torch.tensor(graph[0]['edge_index'], dtype=torch.long)\n",
        "x = torch.tensor(graph[0]['node_feat'], dtype=torch.float)\n",
        "y = torch.tensor(graph[1], dtype=torch.long).squeeze()\n",
        "data = Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "data.edge_index = to_undirected(data.edge_index)\n",
        "data = train_test_split_edges(data, val_ratio=0.05, test_ratio=0.2)\n",
        "\n",
        "# 2. GraphSAGE Encoder\n",
        "class SAGEEncoder(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, 64)\n",
        "        self.conv2 = SAGEConv(64, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# 3. Decoder (inner product)\n",
        "def decode(z, edge_index):\n",
        "    return (z[edge_index[0]] * z[edge_index[1]]).sum(dim=1)\n",
        "\n",
        "# 4. Loss with negative sampling\n",
        "def compute_loss(z, pos_edge_index, num_nodes, batch_size=100000):\n",
        "    pos_score = decode(z, pos_edge_index)\n",
        "    pos_loss = -F.logsigmoid(pos_score).mean()\n",
        "\n",
        "    neg_score_sum = 0\n",
        "    neg_batches = (pos_edge_index.size(1) // batch_size) + 1\n",
        "\n",
        "    for _ in range(neg_batches):\n",
        "        neg_edge_index = negative_sampling(\n",
        "            edge_index=pos_edge_index,\n",
        "            num_nodes=num_nodes,\n",
        "            num_neg_samples=min(batch_size, pos_edge_index.size(1)),\n",
        "        )\n",
        "        neg_score = decode(z, neg_edge_index)\n",
        "        neg_score_sum += -F.logsigmoid(-neg_score).mean()\n",
        "\n",
        "    return pos_loss + neg_score_sum / neg_batches\n",
        "\n",
        "# 5. Train setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = SAGEEncoder(data.num_node_features, 64).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "x = data.x.to(device)\n",
        "train_edge_index = data.train_pos_edge_index.to(device)\n",
        "save_path = \"/content/drive/MyDrive/edge_sage_epoch_{epoch:03d}.pt\"\n",
        "\n",
        "# 6. Train loop\n",
        "for epoch in range(1, 4):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    z = model(x, train_edge_index)\n",
        "    loss = compute_loss(z, train_edge_index, x.size(0))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"[EdgePred - SAGE] Epoch {epoch:02d} | Loss: {loss:.4f}\")\n",
        "    torch.save(model.state_dict(), save_path.format(epoch=epoch))\n"
      ],
      "metadata": {
        "id": "VRMdn5oRSILP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Evaluation\n",
        "@torch.no_grad()\n",
        "def evaluate_edge_prediction(model, x, edge_index, pos_edge_index, neg_edge_index):\n",
        "    model.eval()\n",
        "    z = model(x, edge_index)\n",
        "    pos_scores = torch.sigmoid(decode(z, pos_edge_index)).cpu().numpy()\n",
        "    neg_scores = torch.sigmoid(decode(z, neg_edge_index)).cpu().numpy()\n",
        "    y_true = np.hstack([np.ones(pos_scores.shape[0]), np.zeros(neg_scores.shape[0])])\n",
        "    y_scores = np.hstack([pos_scores, neg_scores])\n",
        "    auc = roc_auc_score(y_true, y_scores)\n",
        "    ap = average_precision_score(y_true, y_scores)\n",
        "    return auc, ap\n",
        "\n",
        "full_edge_index = data.edge_index.to(device)\n",
        "val_pos = data.val_pos_edge_index.to(device)\n",
        "val_neg = negative_sampling(full_edge_index, x.size(0), num_neg_samples=val_pos.size(1))\n",
        "test_pos = data.test_pos_edge_index.to(device)\n",
        "test_neg = negative_sampling(full_edge_index, x.size(0), num_neg_samples=test_pos.size(1))\n",
        "\n",
        "val_auc, val_ap = evaluate_edge_prediction(model, x, full_edge_index, val_pos, val_neg)\n",
        "test_auc, test_ap = evaluate_edge_prediction(model, x, full_edge_index, test_pos, test_neg)\n",
        "\n",
        "print(f\"âœ… Validation AUC: {val_auc:.4f}, AP: {val_ap:.4f}\")\n",
        "print(f\"âœ… Test AUC: {test_auc:.4f}, AP: {test_ap:.4f}\")"
      ],
      "metadata": {
        "id": "K29hn4EESOSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##-----------------------------------------"
      ],
      "metadata": {
        "id": "r0BKLttPEgh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
        "!pip install ogb"
      ],
      "metadata": {
        "id": "G1LGi8v0-CyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZLUBMLbcE85",
        "outputId": "de1164fd-92f9-480a-a818-7e5595b0dbd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.61s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving...\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from ogb.nodeproppred import NodePropPredDataset\n",
        "from torch_geometric.nn import GCNConv, SAGEConv\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# 1. Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
        "dataset = NodePropPredDataset(name='ogbn-products')\n",
        "split_idx = dataset.get_idx_split()\n",
        "\n",
        "graph, labels = dataset[0]\n",
        "edge_index = torch.tensor(graph['edge_index'], dtype=torch.long)\n",
        "x = torch.tensor(graph['node_feat'], dtype=torch.float)\n",
        "y = torch.tensor(labels, dtype=torch.long).squeeze()\n",
        "\n",
        "# Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø§Ø³Ú©â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒØŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ùˆ ØªØ³Øª\n",
        "data = Data(x=x, edge_index=edge_index, y=y)\n",
        "data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
        "data.val_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
        "data.test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
        "\n",
        "data.train_mask[split_idx[\"train\"]] = True\n",
        "data.val_mask[split_idx[\"valid\"]] = True\n",
        "data.test_mask[split_idx[\"test\"]] = True\n",
        "\n",
        "# 2. ØªØ¹Ø±ÛŒÙ Ù…Ø¯Ù„â€ŒÙ‡Ø§\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "        self.dropout = torch.nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
        "        self.dropout = torch.nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# 3. ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ù…ÙˆØ²Ø´\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "data = data.to(device)\n",
        "\n",
        "def train_model(model, data, epochs=100):\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    train_losses, val_accs, val_f1s = [], [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        out = model(data)\n",
        "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "\n",
        "            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¯Ù‚Øª Ùˆ F1 Ø¨Ø±Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ\n",
        "            val_acc = accuracy_score(data.y[data.val_mask].cpu(),\n",
        "                                   pred[data.val_mask].cpu())\n",
        "            val_f1 = f1_score(data.y[data.val_mask].cpu(),\n",
        "                             pred[data.val_mask].cpu(), average='weighted')\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "        val_accs.append(val_acc)\n",
        "        val_f1s.append(val_f1)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, '\n",
        "                  f'Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}')\n",
        "\n",
        "    return model, train_losses, val_accs, val_f1s\n",
        "\n",
        "# 4. Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§\n",
        "print(\"\\nØ¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ GraphSAGE:\")\n",
        "sage_model, sage_loss, sage_val_acc, sage_val_f1 = train_model(\n",
        "    GraphSAGE(data.num_features, 256, dataset.num_classes),\n",
        "    data\n",
        ")\n",
        "\n",
        "print(\"\\nØ¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ GCN:\")\n",
        "gcn_model, gcn_loss, gcn_val_acc, gcn_val_f1 = train_model(\n",
        "    GCN(data.num_features, 256, dataset.num_classes),\n",
        "    data\n",
        ")\n",
        "\n",
        "# 5. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ\n",
        "def evaluate_model(model, data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data)\n",
        "        pred = out.argmax(dim=1)\n",
        "\n",
        "        train_acc = accuracy_score(data.y[data.train_mask].cpu(),\n",
        "                                 pred[data.train_mask].cpu())\n",
        "        val_acc = accuracy_score(data.y[data.val_mask].cpu(),\n",
        "                               pred[data.val_mask].cpu())\n",
        "        test_acc = accuracy_score(data.y[data.test_mask].cpu(),\n",
        "                                pred[data.test_mask].cpu())\n",
        "\n",
        "        test_f1 = f1_score(data.y[data.test_mask].cpu(),\n",
        "                         pred[data.test_mask].cpu(), average='weighted')\n",
        "\n",
        "        return train_acc, val_acc, test_acc, test_f1\n",
        "\n",
        "print(\"\\nØ§Ø±Ø²ÛŒØ§Ø¨ÛŒ GraphSAGE:\")\n",
        "sage_train_acc, sage_val_acc, sage_test_acc, sage_test_f1 = evaluate_model(sage_model, data)\n",
        "print(f\"Train Acc: {sage_train_acc:.4f}, Val Acc: {sage_val_acc:.4f}, \"\n",
        "      f\"Test Acc: {sage_test_acc:.4f}, Test F1: {sage_test_f1:.4f}\")\n",
        "\n",
        "print(\"\\nØ§Ø±Ø²ÛŒØ§Ø¨ÛŒ GCN:\")\n",
        "gcn_train_acc, gcn_val_acc, gcn_test_acc, gcn_test_f1 = evaluate_model(gcn_model, data)\n",
        "print(f\"Train Acc: {gcn_train_acc:.4f}, Val Acc: {gcn_val_acc:.4f}, \"\n",
        "      f\"Test Acc: {gcn_test_acc:.4f}, Test F1: {gcn_test_f1:.4f}\")\n",
        "\n",
        "# 6. Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Ù†Ù…ÙˆØ¯Ø§Ø± Ø®Ø·Ø§\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(sage_loss, label='GraphSAGE')\n",
        "plt.plot(gcn_loss, label='GCN')\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Ù†Ù…ÙˆØ¯Ø§Ø± Ø¯Ù‚Øª Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(sage_val_acc, label='GraphSAGE')\n",
        "plt.plot(gcn_val_acc, label='GCN')\n",
        "plt.title('Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Ù…Ù‚Ø§ÛŒØ³Ù‡ F1\n",
        "plt.subplot(2, 2, 3)\n",
        "models = ['GraphSAGE', 'GCN']\n",
        "test_f1 = [sage_test_f1, gcn_test_f1]\n",
        "plt.bar(models, test_f1, color=['blue', 'orange'])\n",
        "plt.title('Test F1-Score Comparison')\n",
        "plt.ylabel('F1-Score')\n",
        "\n",
        "# Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¯Ù‚Øª ØªØ³Øª\n",
        "plt.subplot(2, 2, 4)\n",
        "test_acc = [sage_test_acc, gcn_test_acc]\n",
        "plt.bar(models, test_acc, color=['blue', 'orange'])\n",
        "plt.title('Test Accuracy Comparison')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('results_comparison.png')\n",
        "plt.show()\n",
        "\n",
        "# 7. Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ø¶Ø§ÙÛŒ: Edge Prediction\n",
        "class EdgePredictor(torch.nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(EdgePredictor, self).__init__()\n",
        "        self.lin1 = torch.nn.Linear(2 * in_channels, 128)\n",
        "        self.lin2 = torch.nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, z, edge_index):\n",
        "        src, dst = edge_index\n",
        "        x = torch.cat([z[src], z[dst]], dim=1)\n",
        "        x = F.relu(self.lin1(x))\n",
        "        return torch.sigmoid(self.lin2(x)).squeeze()\n",
        "\n",
        "def get_embeddings(model, data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ embeddings Ø§Ø² Ù„Ø§ÛŒÙ‡ Ø§ÙˆÙ„\n",
        "        embeddings = model.conv1(data.x, data.edge_index)\n",
        "        embeddings = F.relu(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "print(\"\\nØ¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Edge Prediction Ø¨Ø§ GraphSAGE:\")\n",
        "sage_embeddings = get_embeddings(sage_model, data)\n",
        "edge_model = EdgePredictor(256).to(device)\n",
        "optimizer = torch.optim.Adam(edge_model.parameters(), lr=0.01)\n",
        "\n",
        "# Ù†Ù…ÙˆÙ†Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø² ÛŒØ§Ù„Ù‡Ø§ÛŒ Ù…Ù†ÙÛŒ\n",
        "def negative_sampling(edge_index, num_nodes, num_neg_samples=None):\n",
        "    if num_neg_samples is None:\n",
        "        num_neg_samples = edge_index.size(1)\n",
        "\n",
        "    neg_edge_index = torch.randint(0, num_nodes, (2, num_neg_samples), device=device)\n",
        "    return neg_edge_index\n",
        "\n",
        "for epoch in range(50):\n",
        "    edge_model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø±Ø§ÛŒ ÛŒØ§Ù„Ù‡Ø§ÛŒ Ù…Ø«Ø¨Øª\n",
        "    pos_pred = edge_model(sage_embeddings, data.edge_index)\n",
        "    pos_loss = F.binary_cross_entropy(pos_pred, torch.ones_like(pos_pred))\n",
        "\n",
        "    # Ù†Ù…ÙˆÙ†Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ùˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø±Ø§ÛŒ ÛŒØ§Ù„Ù‡Ø§ÛŒ Ù…Ù†ÙÛŒ\n",
        "    neg_edge_index = negative_sampling(data.edge_index, data.num_nodes, num_neg_samples=data.edge_index.size(1))\n",
        "    neg_pred = edge_model(sage_embeddings, neg_edge_index)\n",
        "    neg_loss = F.binary_cross_entropy(neg_pred, torch.zeros_like(neg_pred))\n",
        "\n",
        "    loss = pos_loss + neg_loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch {epoch+1}/50, Loss: {loss.item():.4f}')\n",
        "\n",
        "# Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Edge Prediction\n",
        "edge_model.eval()\n",
        "with torch.no_grad():\n",
        "    pos_pred = edge_model(sage_embeddings, data.edge_index)\n",
        "    neg_edge_index = negative_sampling(data.edge_index, data.num_nodes, num_neg_samples=100000)\n",
        "    neg_pred = edge_model(sage_embeddings, neg_edge_index)\n",
        "\n",
        "    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¯Ù‚Øª\n",
        "    pos_acc = (pos_pred > 0.5).float().mean()\n",
        "    neg_acc = (neg_pred < 0.5).float().mean()\n",
        "    overall_acc = (pos_acc * pos_pred.size(0) + neg_acc * neg_pred.size(0)) / (pos_pred.size(0) + neg_pred.size(0))\n",
        "\n",
        "    print(f\"\\nÙ†ØªØ§ÛŒØ¬ Edge Prediction:\")\n",
        "    print(f\"Positive Accuracy: {pos_acc.item():.4f}\")\n",
        "    print(f\"Negative Accuracy: {neg_acc.item():.4f}\")\n",
        "    print(f\"Overall Accuracy: {overall_acc.item():.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}